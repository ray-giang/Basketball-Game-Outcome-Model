{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /opt/conda/envs/springboard/lib/python3.7/site-packages (2.10.0)\n",
      "Requirement already satisfied: scikit-optimize in /opt/conda/envs/springboard/lib/python3.7/site-packages (0.8.1)\n",
      "Requirement already satisfied: numpy>=1.7 in /opt/conda/envs/springboard/lib/python3.7/site-packages (from h5py) (1.18.1)\n",
      "Requirement already satisfied: six in /opt/conda/envs/springboard/lib/python3.7/site-packages (from h5py) (1.14.0)\n",
      "Requirement already satisfied: pyaml>=16.9 in /opt/conda/envs/springboard/lib/python3.7/site-packages (from scikit-optimize) (20.4.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/springboard/lib/python3.7/site-packages (from scikit-optimize) (0.14.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/envs/springboard/lib/python3.7/site-packages (from scikit-optimize) (0.22.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/springboard/lib/python3.7/site-packages (from scikit-optimize) (1.4.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/envs/springboard/lib/python3.7/site-packages (from pyaml>=16.9->scikit-optimize) (5.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install h5py scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_histogram, plot_objective_2D\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_game_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "all_season_results_df = df[['team1_id', 'team2_id','game_number_team2', 'win_flag_team1',\n",
    "       'ortg_team1', 'ast%_team1', 'ts%_team1', 'efg%_team1', 'poss_team1',\n",
    "       'dreb%_team1', 'oreb%_team1', 'stl%_team1', 'blk%_team1', 'tov%_team1',\n",
    "       'ft%_team1', 'win%_roll8_team1', 'ortg_team1_opp', 'ast%_team1_opp',\n",
    "       'ts%_team1_opp', 'efg%_team1_opp', 'poss_team1_opp', 'dreb%_team1_opp',\n",
    "       'oreb%_team1_opp', 'stl%_team1_opp', 'blk%_team1_opp', 'tov%_team1_opp',\n",
    "       'ft%_team1_opp', 'win%_team1', 'homeaway_win%_team1',\n",
    "       'b2bbreak_win%_team1', 'travel_win%_team1', 'win%_team1_opp',\n",
    "       'homeaway_win%_team1_opp', 'b2bbreak_win%_team1_opp',\n",
    "       'travel_win%_team1_opp', 'season', 'game_number_team1',\n",
    "       'ortg_team2', 'ast%_team2', 'ts%_team2', 'efg%_team2', 'poss_team2',\n",
    "       'dreb%_team2', 'oreb%_team2', 'stl%_team2', 'blk%_team2', 'tov%_team2',\n",
    "       'ft%_team2', 'win%_roll8_team2', 'ortg_team2_opp', 'ast%_team2_opp',\n",
    "       'ts%_team2_opp', 'efg%_team2_opp', 'poss_team2_opp', 'dreb%_team2_opp',\n",
    "       'oreb%_team2_opp', 'stl%_team2_opp', 'blk%_team2_opp', 'tov%_team2_opp',\n",
    "       'ft%_team2_opp', 'win%_team2', 'homeaway_win%_team2',\n",
    "       'b2bbreak_win%_team2', 'travel_win%_team2', 'win%_team2_opp',\n",
    "       'homeaway_win%_team2_opp', 'b2bbreak_win%_team2_opp',\n",
    "       'travel_win%_team2_opp']]\n",
    "print(len(all_season_results_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team1_id</th>\n",
       "      <th>team2_id</th>\n",
       "      <th>game_number_team2</th>\n",
       "      <th>win_flag_team1</th>\n",
       "      <th>ortg_team1</th>\n",
       "      <th>ast%_team1</th>\n",
       "      <th>ts%_team1</th>\n",
       "      <th>efg%_team1</th>\n",
       "      <th>poss_team1</th>\n",
       "      <th>dreb%_team1</th>\n",
       "      <th>...</th>\n",
       "      <th>tov%_team2_opp</th>\n",
       "      <th>ft%_team2_opp</th>\n",
       "      <th>win%_team2</th>\n",
       "      <th>homeaway_win%_team2</th>\n",
       "      <th>b2bbreak_win%_team2</th>\n",
       "      <th>travel_win%_team2</th>\n",
       "      <th>win%_team2_opp</th>\n",
       "      <th>homeaway_win%_team2_opp</th>\n",
       "      <th>b2bbreak_win%_team2_opp</th>\n",
       "      <th>travel_win%_team2_opp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>114.501981</td>\n",
       "      <td>0.492424</td>\n",
       "      <td>0.578350</td>\n",
       "      <td>0.478073</td>\n",
       "      <td>92.100000</td>\n",
       "      <td>0.660612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>101.498768</td>\n",
       "      <td>0.458718</td>\n",
       "      <td>0.514144</td>\n",
       "      <td>0.421279</td>\n",
       "      <td>91.866667</td>\n",
       "      <td>0.636487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158706</td>\n",
       "      <td>0.812857</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>101.532954</td>\n",
       "      <td>0.465250</td>\n",
       "      <td>0.517012</td>\n",
       "      <td>0.439174</td>\n",
       "      <td>90.300000</td>\n",
       "      <td>0.642365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132306</td>\n",
       "      <td>0.725852</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>103.498078</td>\n",
       "      <td>0.495277</td>\n",
       "      <td>0.521044</td>\n",
       "      <td>0.451339</td>\n",
       "      <td>90.200000</td>\n",
       "      <td>0.690636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203136</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>99.505974</td>\n",
       "      <td>0.508885</td>\n",
       "      <td>0.502993</td>\n",
       "      <td>0.438616</td>\n",
       "      <td>89.833333</td>\n",
       "      <td>0.699186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170958</td>\n",
       "      <td>0.709501</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   team1_id  team2_id  game_number_team2  win_flag_team1  ortg_team1  \\\n",
       "0         1        20                  2               0  114.501981   \n",
       "1         1         6                  3               1  101.498768   \n",
       "2         1         5                  5               1  101.532954   \n",
       "3         1        26                  5               0  103.498078   \n",
       "4         1         6                  6               1   99.505974   \n",
       "\n",
       "   ast%_team1  ts%_team1  efg%_team1  poss_team1  dreb%_team1  ...  \\\n",
       "0    0.492424   0.578350    0.478073   92.100000     0.660612  ...   \n",
       "1    0.458718   0.514144    0.421279   91.866667     0.636487  ...   \n",
       "2    0.465250   0.517012    0.439174   90.300000     0.642365  ...   \n",
       "3    0.495277   0.521044    0.451339   90.200000     0.690636  ...   \n",
       "4    0.508885   0.502993    0.438616   89.833333     0.699186  ...   \n",
       "\n",
       "   tov%_team2_opp  ft%_team2_opp  win%_team2  homeaway_win%_team2  \\\n",
       "0        0.161290       0.800000        0.00             0.000000   \n",
       "1        0.158706       0.812857        0.50             0.000000   \n",
       "2        0.132306       0.725852        0.25             0.000000   \n",
       "3        0.203136       0.593750        0.75             0.666667   \n",
       "4        0.170958       0.709501        0.20             0.000000   \n",
       "\n",
       "   b2bbreak_win%_team2  travel_win%_team2  win%_team2_opp  \\\n",
       "0             0.000000                0.0        1.000000   \n",
       "1             0.500000                0.5        0.666667   \n",
       "2             0.000000                0.0        0.750000   \n",
       "3             0.666667                0.5        0.800000   \n",
       "4             0.200000                0.0        0.666667   \n",
       "\n",
       "   homeaway_win%_team2_opp  b2bbreak_win%_team2_opp  travel_win%_team2_opp  \n",
       "0                 0.000000                     1.00               0.000000  \n",
       "1                 0.000000                     1.00               0.000000  \n",
       "2                 1.000000                     1.00               0.500000  \n",
       "3                 0.500000                     1.00               0.666667  \n",
       "4                 0.333333                     0.75               0.500000  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_season_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of arrays that represent the combination of parameters we will tune for   \n",
    "\n",
    "dim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_num_dense_layers = Integer(low=1, high=3, name='num_dense_layers')\n",
    "dim_num_dense_nodes = Integer(low=2, high=48, name='num_dense_nodes')\n",
    "dim_activation = Categorical(categories=['relu', 'elu'],\n",
    "                             name='activation')\n",
    "hyperparameters = [dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_activation]                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training and test splits for cross validation purposes\n",
    "X = all_season_results_df.drop(['win_flag_team1'], axis=1)\n",
    "Y = all_season_results_df.win_flag_team1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = (X_test,Y_test)\n",
    "validation_data\n",
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dir_name(learning_rate, num_dense_layers,\n",
    "                 num_dense_nodes, activation):\n",
    "\n",
    "    # The dir-name for the TensorBoard log-dir.\n",
    "    s = \"./19_logs/lr_{0:.0e}_layers_{1}_nodes_{2}_{3}/\"\n",
    "\n",
    "    # Insert all the hyper-parameters in the dir-name.\n",
    "    log_dir = s.format(learning_rate,\n",
    "                       num_dense_layers,\n",
    "                       num_dense_nodes,\n",
    "                       activation\n",
    "                       )\n",
    "\n",
    "    return log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate, num_dense_layers,\n",
    "                 num_dense_nodes, activation):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    kernel_initializer: Setting the initial random weights of Keras layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start construction of a Keras Sequential model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an input layer which is similar to a feed_dict in TensorFlow.\n",
    "    # Note that the input-shape must be a tuple containing the image-size.\n",
    "    model.add(Dense(len(X_train.columns),\n",
    "                    input_dim = len(X_train.columns),\n",
    "                    activation = 'relu'\n",
    "                    ))\n",
    "\n",
    "    # Add fully-connected / dense layers.\n",
    "    # The number of layers is a hyper-parameter we want to optimize.\n",
    "    for i in range(num_dense_layers):\n",
    "        # Name of the layer. This is not really necessary\n",
    "        # because Keras should give them unique names.\n",
    "        name = 'layer_dense_{0}'.format(i+1)\n",
    "\n",
    "        # Add the dense / fully-connected layer to the model.\n",
    "        # This has two hyper-parameters we want to optimize:\n",
    "        # The number of nodes and the activation function.\n",
    "        model.add(Dense(num_dense_nodes,\n",
    "                        activation=activation,\n",
    "                        name=name))\n",
    "\n",
    "    # Last fully-connected / dense layer with 'sigmoid'\n",
    "    # for use in classification.\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Use the Adam method for training the network.\n",
    "    # We want to find the best learning-rate for the Adam method.\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    # In Keras we need to compile the model so it can be trained.\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_best_model = 'SB_capstone_bestpathmodel.h5'\n",
    "@use_named_args(dimensions=hyperparameters)\n",
    "def fitness(learning_rate, num_dense_layers,\n",
    "            num_dense_nodes, activation):\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('num_dense_layers:', num_dense_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('activation:', activation)\n",
    "    \n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = create_model(learning_rate=learning_rate,\n",
    "                         num_dense_layers=num_dense_layers,\n",
    "                         num_dense_nodes=num_dense_nodes,\n",
    "                         activation=activation)\n",
    "\n",
    "    # Dir-name for the TensorBoard log-files.\n",
    "    log_dir = log_dir_name(learning_rate, num_dense_layers,\n",
    "                           num_dense_nodes, activation)\n",
    "    \n",
    "    # Create a callback-function for Keras which will be\n",
    "    # run after each epoch has ended during training.\n",
    "    # This saves the log-files for TensorBoard.\n",
    "    callback_log = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=False)\n",
    "   \n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=Y_train,\n",
    "                        epochs=25,\n",
    "                        batch_size=5,\n",
    "                        validation_data=validation_data,\n",
    "                        callbacks=[callback_log])\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history.history['val_acc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_accuracy\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if accuracy > best_accuracy:\n",
    "        # Save the new model to harddisk.\n",
    "        model.save(path_best_model)\n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 1.0e-05\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 16\n",
      "activation: relu\n",
      "WARNING:tensorflow:From /opt/conda/envs/springboard/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/conda/envs/springboard/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 3.4005 - acc: 0.4898 - val_loss: 1.0143 - val_acc: 0.4905\n",
      "\n",
      "Accuracy: 49.05%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.49048418"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_parameters = [1e-5, 1, 16, 'relu']\n",
    "fitness(x=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 1.0e-05\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 16\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 279us/sample - loss: 13.7968 - acc: 0.4873 - val_loss: 1.3243 - val_acc: 0.4665\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 277us/sample - loss: 1.0063 - acc: 0.4548 - val_loss: 0.9120 - val_acc: 0.4506\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.8817 - acc: 0.4640 - val_loss: 0.8576 - val_acc: 0.4756\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.8233 - acc: 0.4911 - val_loss: 0.7957 - val_acc: 0.4932\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 275us/sample - loss: 0.7794 - acc: 0.5118 - val_loss: 0.7571 - val_acc: 0.5191\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.7464 - acc: 0.5334 - val_loss: 0.7361 - val_acc: 0.5424\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 275us/sample - loss: 0.7218 - acc: 0.5535 - val_loss: 0.7249 - val_acc: 0.5507\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.7037 - acc: 0.5698 - val_loss: 0.7290 - val_acc: 0.5567\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.6882 - acc: 0.5847 - val_loss: 0.6744 - val_acc: 0.5939\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6766 - acc: 0.6003 - val_loss: 0.6762 - val_acc: 0.6035\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 275us/sample - loss: 0.6711 - acc: 0.6062 - val_loss: 0.6560 - val_acc: 0.6184\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6642 - acc: 0.6153 - val_loss: 0.6527 - val_acc: 0.6225\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 276us/sample - loss: 0.6591 - acc: 0.6192 - val_loss: 0.6469 - val_acc: 0.6300\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 276us/sample - loss: 0.6551 - acc: 0.6270 - val_loss: 0.6457 - val_acc: 0.6342\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6529 - acc: 0.6284 - val_loss: 0.6426 - val_acc: 0.6377\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 277us/sample - loss: 0.6492 - acc: 0.6347 - val_loss: 0.6384 - val_acc: 0.6433\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.6483 - acc: 0.6356 - val_loss: 0.6406 - val_acc: 0.6385\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 277us/sample - loss: 0.6463 - acc: 0.6352 - val_loss: 0.6537 - val_acc: 0.6248\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.6446 - acc: 0.6397 - val_loss: 0.6333 - val_acc: 0.6475\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 278us/sample - loss: 0.6434 - acc: 0.6392 - val_loss: 0.6418 - val_acc: 0.6369\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 278us/sample - loss: 0.6422 - acc: 0.6394 - val_loss: 0.6403 - val_acc: 0.6397\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 276us/sample - loss: 0.6420 - acc: 0.6427 - val_loss: 0.6415 - val_acc: 0.6370\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 276us/sample - loss: 0.6419 - acc: 0.6429 - val_loss: 0.6301 - val_acc: 0.6526\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 275us/sample - loss: 0.6397 - acc: 0.6437 - val_loss: 0.6334 - val_acc: 0.6468\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.6406 - acc: 0.6421 - val_loss: 0.6275 - val_acc: 0.6549\n",
      "\n",
      "Accuracy: 65.49%\n",
      "\n",
      "learning rate: 3.5e-03\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 33\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.7360 - acc: 0.5011 - val_loss: 0.6992 - val_acc: 0.4984\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6986 - acc: 0.4986 - val_loss: 0.6947 - val_acc: 0.5016\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6987 - acc: 0.5007 - val_loss: 0.6935 - val_acc: 0.5016\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6978 - acc: 0.5017 - val_loss: 0.6944 - val_acc: 0.5016\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6980 - acc: 0.4967 - val_loss: 0.6941 - val_acc: 0.5016\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6983 - acc: 0.4975 - val_loss: 0.6936 - val_acc: 0.4984\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6980 - acc: 0.4998 - val_loss: 0.6933 - val_acc: 0.5016\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6983 - acc: 0.4971 - val_loss: 0.6934 - val_acc: 0.5016\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6985 - acc: 0.4939 - val_loss: 0.6943 - val_acc: 0.4984\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6982 - acc: 0.5008 - val_loss: 0.6943 - val_acc: 0.4984\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6983 - acc: 0.5016 - val_loss: 0.6960 - val_acc: 0.4984\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6984 - acc: 0.5025 - val_loss: 0.7040 - val_acc: 0.5016\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6980 - acc: 0.5034 - val_loss: 0.6940 - val_acc: 0.4984\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6987 - acc: 0.4970 - val_loss: 0.6934 - val_acc: 0.5016\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6980 - acc: 0.5002 - val_loss: 0.6948 - val_acc: 0.4984\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6983 - acc: 0.5000 - val_loss: 0.6947 - val_acc: 0.5016\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6974 - acc: 0.5014 - val_loss: 0.7080 - val_acc: 0.5016\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6981 - acc: 0.4991 - val_loss: 0.6934 - val_acc: 0.4984\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6980 - acc: 0.5040 - val_loss: 0.6944 - val_acc: 0.5016\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6984 - acc: 0.4988 - val_loss: 0.6936 - val_acc: 0.5016\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6982 - acc: 0.5041 - val_loss: 0.6938 - val_acc: 0.5016\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6984 - acc: 0.5019 - val_loss: 0.6940 - val_acc: 0.5016\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6976 - acc: 0.5016 - val_loss: 0.6945 - val_acc: 0.4984\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6981 - acc: 0.5022 - val_loss: 0.6974 - val_acc: 0.5016\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6979 - acc: 0.5036 - val_loss: 0.7050 - val_acc: 0.5016\n",
      "\n",
      "Accuracy: 50.16%\n",
      "\n",
      "learning rate: 3.9e-06\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 41\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 320us/sample - loss: 3.3219 - acc: 0.5199 - val_loss: 0.8549 - val_acc: 0.5283\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.7826 - acc: 0.5308 - val_loss: 0.7675 - val_acc: 0.5317\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 13s 318us/sample - loss: 0.7423 - acc: 0.5375 - val_loss: 0.7160 - val_acc: 0.5522\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 13s 315us/sample - loss: 0.7158 - acc: 0.5517 - val_loss: 0.7030 - val_acc: 0.5623\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 13s 315us/sample - loss: 0.7023 - acc: 0.5588 - val_loss: 0.6988 - val_acc: 0.5649\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 13s 316us/sample - loss: 0.6909 - acc: 0.5679 - val_loss: 0.6794 - val_acc: 0.5738\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 13s 317us/sample - loss: 0.6833 - acc: 0.5730 - val_loss: 0.6762 - val_acc: 0.5873\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 0.6764 - acc: 0.5811 - val_loss: 0.6671 - val_acc: 0.5972\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6702 - acc: 0.5915 - val_loss: 0.6742 - val_acc: 0.5824\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6657 - acc: 0.5981 - val_loss: 0.6565 - val_acc: 0.6134\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 0.6619 - acc: 0.6037 - val_loss: 0.6551 - val_acc: 0.6147\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 0.6587 - acc: 0.6082 - val_loss: 0.6610 - val_acc: 0.6008\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 13s 315us/sample - loss: 0.6560 - acc: 0.6124 - val_loss: 0.6459 - val_acc: 0.6289\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6527 - acc: 0.6165 - val_loss: 0.6455 - val_acc: 0.6276\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 13s 319us/sample - loss: 0.6505 - acc: 0.6198 - val_loss: 0.6433 - val_acc: 0.6321\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6489 - acc: 0.6231 - val_loss: 0.6404 - val_acc: 0.6385\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6475 - acc: 0.6243 - val_loss: 0.6530 - val_acc: 0.6150\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6456 - acc: 0.6268 - val_loss: 0.6371 - val_acc: 0.6401\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6434 - acc: 0.6292 - val_loss: 0.6388 - val_acc: 0.6404\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 13s 315us/sample - loss: 0.6432 - acc: 0.6328 - val_loss: 0.6406 - val_acc: 0.6366\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6428 - acc: 0.6314 - val_loss: 0.6339 - val_acc: 0.6431\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 13s 315us/sample - loss: 0.6412 - acc: 0.6349 - val_loss: 0.6331 - val_acc: 0.6441\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6395 - acc: 0.6374 - val_loss: 0.6357 - val_acc: 0.6444\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6391 - acc: 0.6374 - val_loss: 0.6308 - val_acc: 0.6480\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6389 - acc: 0.6391 - val_loss: 0.6309 - val_acc: 0.6494\n",
      "\n",
      "Accuracy: 64.94%\n",
      "\n",
      "learning rate: 9.8e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 35\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 1.0709 - acc: 0.5322 - val_loss: 0.9804 - val_acc: 0.5016\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6969 - acc: 0.5713 - val_loss: 0.6565 - val_acc: 0.6029\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6699 - acc: 0.5939 - val_loss: 0.6579 - val_acc: 0.5966\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6612 - acc: 0.6072 - val_loss: 0.6317 - val_acc: 0.6463\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6518 - acc: 0.6165 - val_loss: 0.6308 - val_acc: 0.6480\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6464 - acc: 0.6253 - val_loss: 0.6411 - val_acc: 0.6453\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6416 - acc: 0.6325 - val_loss: 0.6285 - val_acc: 0.6474\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.6414 - acc: 0.6310 - val_loss: 0.6363 - val_acc: 0.6350\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6384 - acc: 0.6358 - val_loss: 0.6467 - val_acc: 0.6171\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6356 - acc: 0.6370 - val_loss: 0.6285 - val_acc: 0.6457\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6352 - acc: 0.6412 - val_loss: 0.6422 - val_acc: 0.6299\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6333 - acc: 0.6444 - val_loss: 0.6231 - val_acc: 0.6513\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6319 - acc: 0.6454 - val_loss: 0.6219 - val_acc: 0.6555\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.6315 - acc: 0.6419 - val_loss: 0.6200 - val_acc: 0.6572\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6313 - acc: 0.6442 - val_loss: 0.6267 - val_acc: 0.6516\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6301 - acc: 0.6429 - val_loss: 0.6210 - val_acc: 0.6586\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.6291 - acc: 0.6457 - val_loss: 0.6171 - val_acc: 0.6588\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6286 - acc: 0.6474 - val_loss: 0.6233 - val_acc: 0.6528\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6275 - acc: 0.6473 - val_loss: 0.6208 - val_acc: 0.6532\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6283 - acc: 0.6474 - val_loss: 0.6157 - val_acc: 0.6562\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6275 - acc: 0.6483 - val_loss: 0.6149 - val_acc: 0.6575\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6264 - acc: 0.6485 - val_loss: 0.6188 - val_acc: 0.6569\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6264 - acc: 0.6507 - val_loss: 0.6197 - val_acc: 0.6568\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6264 - acc: 0.6493 - val_loss: 0.6357 - val_acc: 0.6348\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6254 - acc: 0.6500 - val_loss: 0.6208 - val_acc: 0.6544\n",
      "\n",
      "Accuracy: 65.44%\n",
      "\n",
      "learning rate: 4.9e-03\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 8\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.7014 - acc: 0.5038 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6935 - acc: 0.4993 - val_loss: 0.6933 - val_acc: 0.5016\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6934 - acc: 0.5021 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6935 - acc: 0.4970 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6935 - acc: 0.4991 - val_loss: 0.6936 - val_acc: 0.4984\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6934 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6934 - acc: 0.5015 - val_loss: 0.6935 - val_acc: 0.5016\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6934 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6935 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6934 - acc: 0.5018 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6935 - acc: 0.5020 - val_loss: 0.6934 - val_acc: 0.4984\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6933 - acc: 0.4994 - val_loss: 0.6933 - val_acc: 0.5016\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6934 - acc: 0.5003 - val_loss: 0.6936 - val_acc: 0.5016\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6935 - acc: 0.5027 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6935 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6934 - acc: 0.5000 - val_loss: 0.6935 - val_acc: 0.4984\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6934 - acc: 0.5043 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6935 - acc: 0.4987 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6935 - acc: 0.4995 - val_loss: 0.6933 - val_acc: 0.4984\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6935 - acc: 0.4956 - val_loss: 0.6934 - val_acc: 0.4984\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6933 - acc: 0.5038 - val_loss: 0.6937 - val_acc: 0.5016\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6935 - acc: 0.4970 - val_loss: 0.6935 - val_acc: 0.5016\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6935 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6934 - acc: 0.5018 - val_loss: 0.6937 - val_acc: 0.4984\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6934 - acc: 0.5040 - val_loss: 0.6936 - val_acc: 0.5016\n",
      "\n",
      "Accuracy: 50.16%\n",
      "\n",
      "learning rate: 1.6e-03\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 16\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 1.4410 - acc: 0.5557 - val_loss: 1.4081 - val_acc: 0.4984\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.7043 - acc: 0.5169 - val_loss: 0.6934 - val_acc: 0.4984\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6947 - acc: 0.4961 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6943 - acc: 0.5053 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6947 - acc: 0.4985 - val_loss: 0.6942 - val_acc: 0.5016\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 261us/sample - loss: 0.6948 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6944 - acc: 0.5021 - val_loss: 0.6935 - val_acc: 0.5016\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6947 - acc: 0.4984 - val_loss: 0.6933 - val_acc: 0.4984\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6946 - acc: 0.5008 - val_loss: 0.6949 - val_acc: 0.4984\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6948 - acc: 0.4975 - val_loss: 0.6963 - val_acc: 0.4984\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6947 - acc: 0.4988 - val_loss: 0.6946 - val_acc: 0.5016\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6946 - acc: 0.4982 - val_loss: 0.6941 - val_acc: 0.5016\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6946 - acc: 0.5008 - val_loss: 0.6936 - val_acc: 0.5016\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6947 - acc: 0.4967 - val_loss: 0.6944 - val_acc: 0.4984\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 260us/sample - loss: 0.6947 - acc: 0.4995 - val_loss: 0.6941 - val_acc: 0.4984\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6944 - acc: 0.5024 - val_loss: 0.6946 - val_acc: 0.4984\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6948 - acc: 0.4970 - val_loss: 0.6934 - val_acc: 0.5016\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6946 - acc: 0.5012 - val_loss: 0.6934 - val_acc: 0.5016\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6946 - acc: 0.4986 - val_loss: 0.6950 - val_acc: 0.5016\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6947 - acc: 0.4973 - val_loss: 0.6952 - val_acc: 0.4984\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6947 - acc: 0.5031 - val_loss: 0.6947 - val_acc: 0.4984\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6948 - acc: 0.4960 - val_loss: 0.6967 - val_acc: 0.5016\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6946 - acc: 0.5011 - val_loss: 0.6944 - val_acc: 0.5016\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 262us/sample - loss: 0.6945 - acc: 0.5027 - val_loss: 0.6936 - val_acc: 0.4984\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6945 - acc: 0.5011 - val_loss: 0.6957 - val_acc: 0.4984\n",
      "\n",
      "Accuracy: 49.84%\n",
      "\n",
      "learning rate: 7.0e-06\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 26\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 1.4579 - acc: 0.4945 - val_loss: 0.8621 - val_acc: 0.4999\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.7955 - acc: 0.5133 - val_loss: 0.7467 - val_acc: 0.5253\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.7330 - acc: 0.5451 - val_loss: 0.6970 - val_acc: 0.5680\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.7017 - acc: 0.5669 - val_loss: 0.6747 - val_acc: 0.5907\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6826 - acc: 0.5882 - val_loss: 0.6692 - val_acc: 0.5989\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6732 - acc: 0.5976 - val_loss: 0.6526 - val_acc: 0.6229\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6629 - acc: 0.6128 - val_loss: 0.7549 - val_acc: 0.5429\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6597 - acc: 0.6166 - val_loss: 0.6440 - val_acc: 0.6308\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6561 - acc: 0.6196 - val_loss: 0.6336 - val_acc: 0.6436\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6527 - acc: 0.6266 - val_loss: 0.6340 - val_acc: 0.6433\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6512 - acc: 0.6268 - val_loss: 0.6790 - val_acc: 0.6014\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6483 - acc: 0.6288 - val_loss: 0.6456 - val_acc: 0.6349\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6464 - acc: 0.6341 - val_loss: 0.6678 - val_acc: 0.6083\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6482 - acc: 0.6316 - val_loss: 0.6293 - val_acc: 0.6485\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6475 - acc: 0.6346 - val_loss: 0.6827 - val_acc: 0.6030\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6451 - acc: 0.6360 - val_loss: 0.6317 - val_acc: 0.6452\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6440 - acc: 0.6355 - val_loss: 0.6326 - val_acc: 0.6450\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6432 - acc: 0.6381 - val_loss: 0.6490 - val_acc: 0.6335\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6440 - acc: 0.6385 - val_loss: 0.6392 - val_acc: 0.6413\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6430 - acc: 0.6398 - val_loss: 0.6352 - val_acc: 0.6443\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6434 - acc: 0.6425 - val_loss: 0.6255 - val_acc: 0.6523\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6428 - acc: 0.6389 - val_loss: 0.6244 - val_acc: 0.6544\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6429 - acc: 0.6375 - val_loss: 0.6303 - val_acc: 0.6477\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6389 - acc: 0.6421 - val_loss: 0.6270 - val_acc: 0.6539\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6413 - acc: 0.6413 - val_loss: 0.6235 - val_acc: 0.6549\n",
      "\n",
      "Accuracy: 65.49%\n",
      "\n",
      "learning rate: 1.2e-05\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 24\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 12s 300us/sample - loss: 0.7640 - acc: 0.4920 - val_loss: 0.7186 - val_acc: 0.5013\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 290us/sample - loss: 0.7091 - acc: 0.5116 - val_loss: 0.6926 - val_acc: 0.5370\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 290us/sample - loss: 0.6947 - acc: 0.5382 - val_loss: 0.6810 - val_acc: 0.5648\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6846 - acc: 0.5608 - val_loss: 0.6999 - val_acc: 0.5245\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 287us/sample - loss: 0.6753 - acc: 0.5820 - val_loss: 0.6634 - val_acc: 0.6076\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 288us/sample - loss: 0.6681 - acc: 0.5928 - val_loss: 0.6625 - val_acc: 0.6003\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6619 - acc: 0.6026 - val_loss: 0.6832 - val_acc: 0.5552\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6576 - acc: 0.6126 - val_loss: 0.6767 - val_acc: 0.5686\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6530 - acc: 0.6159 - val_loss: 0.6480 - val_acc: 0.6252\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 288us/sample - loss: 0.6511 - acc: 0.6177 - val_loss: 0.6430 - val_acc: 0.6361\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6479 - acc: 0.6263 - val_loss: 0.6461 - val_acc: 0.6258\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6450 - acc: 0.6293 - val_loss: 0.6428 - val_acc: 0.6303\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 12s 289us/sample - loss: 0.6441 - acc: 0.6302 - val_loss: 0.6330 - val_acc: 0.6494\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 288us/sample - loss: 0.6432 - acc: 0.6338 - val_loss: 0.6410 - val_acc: 0.6320\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 283us/sample - loss: 0.6410 - acc: 0.6336 - val_loss: 0.6361 - val_acc: 0.6428\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6399 - acc: 0.6376 - val_loss: 0.6298 - val_acc: 0.6533\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6395 - acc: 0.6384 - val_loss: 0.6339 - val_acc: 0.6460\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 291us/sample - loss: 0.6387 - acc: 0.6374 - val_loss: 0.6284 - val_acc: 0.6556\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6383 - acc: 0.6402 - val_loss: 0.6282 - val_acc: 0.6534\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 288us/sample - loss: 0.6378 - acc: 0.6415 - val_loss: 0.6293 - val_acc: 0.6517\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 287us/sample - loss: 0.6362 - acc: 0.6440 - val_loss: 0.6298 - val_acc: 0.6495\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 287us/sample - loss: 0.6379 - acc: 0.6398 - val_loss: 0.6296 - val_acc: 0.6504\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 288us/sample - loss: 0.6358 - acc: 0.6425 - val_loss: 0.6259 - val_acc: 0.6549\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6359 - acc: 0.6421 - val_loss: 0.6678 - val_acc: 0.6038\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6348 - acc: 0.6447 - val_loss: 0.6442 - val_acc: 0.6284\n",
      "\n",
      "Accuracy: 62.84%\n",
      "\n",
      "learning rate: 5.3e-06\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 5\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 278us/sample - loss: 1.3242 - acc: 0.4955 - val_loss: 0.7358 - val_acc: 0.4988\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.7282 - acc: 0.5043 - val_loss: 0.7222 - val_acc: 0.5070\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.7158 - acc: 0.5123 - val_loss: 0.7119 - val_acc: 0.5173\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.7070 - acc: 0.5244 - val_loss: 0.7035 - val_acc: 0.5284\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6998 - acc: 0.5332 - val_loss: 0.6972 - val_acc: 0.5362\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6942 - acc: 0.5427 - val_loss: 0.6918 - val_acc: 0.5469\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6890 - acc: 0.5510 - val_loss: 0.6865 - val_acc: 0.5582\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6842 - acc: 0.5641 - val_loss: 0.6832 - val_acc: 0.5712\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.6803 - acc: 0.5688 - val_loss: 0.6779 - val_acc: 0.5776\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6761 - acc: 0.5746 - val_loss: 0.6742 - val_acc: 0.5831\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6729 - acc: 0.5834 - val_loss: 0.6716 - val_acc: 0.5930\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6696 - acc: 0.5897 - val_loss: 0.6677 - val_acc: 0.5923\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6667 - acc: 0.5940 - val_loss: 0.6655 - val_acc: 0.5950\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6637 - acc: 0.5996 - val_loss: 0.6652 - val_acc: 0.5923\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 275us/sample - loss: 0.6616 - acc: 0.6060 - val_loss: 0.6635 - val_acc: 0.5957\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6591 - acc: 0.6094 - val_loss: 0.6573 - val_acc: 0.6127\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6564 - acc: 0.6152 - val_loss: 0.6553 - val_acc: 0.6227\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.6542 - acc: 0.6185 - val_loss: 0.6525 - val_acc: 0.6245\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6522 - acc: 0.6219 - val_loss: 0.6620 - val_acc: 0.5960\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6502 - acc: 0.6261 - val_loss: 0.6490 - val_acc: 0.6268\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 276us/sample - loss: 0.6488 - acc: 0.6257 - val_loss: 0.6513 - val_acc: 0.6285\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.6470 - acc: 0.6286 - val_loss: 0.6453 - val_acc: 0.6303\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6454 - acc: 0.6340 - val_loss: 0.6453 - val_acc: 0.6310\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.6442 - acc: 0.6319 - val_loss: 0.6426 - val_acc: 0.6358\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 275us/sample - loss: 0.6428 - acc: 0.6359 - val_loss: 0.6409 - val_acc: 0.6375\n",
      "\n",
      "Accuracy: 63.75%\n",
      "\n",
      "learning rate: 1.8e-04\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 47\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 0.9495 - acc: 0.5373 - val_loss: 1.7557 - val_acc: 0.4984\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.7536 - acc: 0.5667 - val_loss: 0.7223 - val_acc: 0.5241\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6837 - acc: 0.5745 - val_loss: 0.6618 - val_acc: 0.5961\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6800 - acc: 0.5670 - val_loss: 0.6683 - val_acc: 0.5561\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6598 - acc: 0.6071 - val_loss: 0.6342 - val_acc: 0.6426\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6503 - acc: 0.6223 - val_loss: 0.6298 - val_acc: 0.6540\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 0.6441 - acc: 0.6299 - val_loss: 0.6246 - val_acc: 0.6526\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6432 - acc: 0.6335 - val_loss: 0.6382 - val_acc: 0.6330\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6374 - acc: 0.6373 - val_loss: 0.6222 - val_acc: 0.6570\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6359 - acc: 0.6401 - val_loss: 0.6332 - val_acc: 0.6387\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6342 - acc: 0.6410 - val_loss: 0.6234 - val_acc: 0.6529\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6335 - acc: 0.6443 - val_loss: 0.6305 - val_acc: 0.6436\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 0.6321 - acc: 0.6435 - val_loss: 0.6440 - val_acc: 0.6272\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6329 - acc: 0.6457 - val_loss: 0.6358 - val_acc: 0.6386\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 13s 316us/sample - loss: 0.6295 - acc: 0.6459 - val_loss: 0.6199 - val_acc: 0.6525\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6292 - acc: 0.6469 - val_loss: 0.6230 - val_acc: 0.6509\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6292 - acc: 0.6446 - val_loss: 0.6250 - val_acc: 0.6478\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 0.6274 - acc: 0.6494 - val_loss: 0.6205 - val_acc: 0.6594\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6275 - acc: 0.6500 - val_loss: 0.6506 - val_acc: 0.6228\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6276 - acc: 0.6478 - val_loss: 0.6500 - val_acc: 0.6149\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6265 - acc: 0.6504 - val_loss: 0.6170 - val_acc: 0.6584\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6264 - acc: 0.6488 - val_loss: 0.6145 - val_acc: 0.6586\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 13s 316us/sample - loss: 0.6251 - acc: 0.6514 - val_loss: 0.6286 - val_acc: 0.6443\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6247 - acc: 0.6508 - val_loss: 0.6307 - val_acc: 0.6364\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6250 - acc: 0.6486 - val_loss: 0.6314 - val_acc: 0.6447\n",
      "\n",
      "Accuracy: 64.47%\n",
      "\n",
      "learning rate: 4.1e-03\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 21\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 12s 291us/sample - loss: 0.7676 - acc: 0.4941 - val_loss: 0.6948 - val_acc: 0.5016\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6961 - acc: 0.5064 - val_loss: 0.6967 - val_acc: 0.4984\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6962 - acc: 0.4991 - val_loss: 0.6933 - val_acc: 0.5016\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 288us/sample - loss: 0.6985 - acc: 0.4993 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6966 - acc: 0.4957 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6968 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6968 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 292us/sample - loss: 0.6963 - acc: 0.5044 - val_loss: 0.6970 - val_acc: 0.4984\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 287us/sample - loss: 0.6972 - acc: 0.5005 - val_loss: 0.6936 - val_acc: 0.5016\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6970 - acc: 0.4965 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6974 - acc: 0.5048 - val_loss: 0.6964 - val_acc: 0.5016\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 289us/sample - loss: 0.6984 - acc: 0.4928 - val_loss: 0.7180 - val_acc: 0.4984\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 283us/sample - loss: 0.6974 - acc: 0.5014 - val_loss: 0.6994 - val_acc: 0.4984\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6974 - acc: 0.5013 - val_loss: 0.6946 - val_acc: 0.5016\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6973 - acc: 0.4989 - val_loss: 0.6944 - val_acc: 0.4984\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 281us/sample - loss: 0.6972 - acc: 0.5052 - val_loss: 0.6985 - val_acc: 0.4984\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6972 - acc: 0.5024 - val_loss: 0.6954 - val_acc: 0.4984\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6976 - acc: 0.5000 - val_loss: 0.6964 - val_acc: 0.5016\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6974 - acc: 0.5004 - val_loss: 0.6948 - val_acc: 0.4984\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6973 - acc: 0.4955 - val_loss: 0.6947 - val_acc: 0.4984\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6979 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6978 - acc: 0.4966 - val_loss: 0.6959 - val_acc: 0.5016\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 283us/sample - loss: 0.6977 - acc: 0.4981 - val_loss: 0.7020 - val_acc: 0.4984\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6974 - acc: 0.5006 - val_loss: 0.6965 - val_acc: 0.5016\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6970 - acc: 0.5030 - val_loss: 0.7043 - val_acc: 0.5016\n",
      "\n",
      "Accuracy: 50.16%\n",
      "\n",
      "learning rate: 1.0e-06\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 48\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 276us/sample - loss: 11.4040 - acc: 0.4923 - val_loss: 0.8578 - val_acc: 0.4849\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.8385 - acc: 0.4862 - val_loss: 0.8270 - val_acc: 0.4862\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.8244 - acc: 0.4887 - val_loss: 0.8177 - val_acc: 0.4895\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.8159 - acc: 0.4910 - val_loss: 0.8086 - val_acc: 0.4941\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.8083 - acc: 0.4943 - val_loss: 0.8016 - val_acc: 0.4967\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.8017 - acc: 0.4970 - val_loss: 0.8007 - val_acc: 0.4984\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7952 - acc: 0.5008 - val_loss: 0.7940 - val_acc: 0.4999\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7891 - acc: 0.5031 - val_loss: 0.7883 - val_acc: 0.5021\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7835 - acc: 0.5062 - val_loss: 0.7810 - val_acc: 0.5058\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.7784 - acc: 0.5080 - val_loss: 0.7753 - val_acc: 0.5094\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.7723 - acc: 0.5139 - val_loss: 0.7680 - val_acc: 0.5116\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.7668 - acc: 0.5152 - val_loss: 0.7690 - val_acc: 0.5156\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.7617 - acc: 0.5197 - val_loss: 0.7581 - val_acc: 0.5181\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7568 - acc: 0.5195 - val_loss: 0.7571 - val_acc: 0.5194\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.7518 - acc: 0.5238 - val_loss: 0.7581 - val_acc: 0.5266\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7473 - acc: 0.5265 - val_loss: 0.7445 - val_acc: 0.5268\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.7430 - acc: 0.5291 - val_loss: 0.7425 - val_acc: 0.5310\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.7392 - acc: 0.5330 - val_loss: 0.7367 - val_acc: 0.5356\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7352 - acc: 0.5373 - val_loss: 0.7333 - val_acc: 0.5365\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.7317 - acc: 0.5395 - val_loss: 0.7296 - val_acc: 0.5400\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.7281 - acc: 0.5440 - val_loss: 0.7257 - val_acc: 0.5429\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.7246 - acc: 0.5445 - val_loss: 0.7227 - val_acc: 0.5481\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7215 - acc: 0.5481 - val_loss: 0.7198 - val_acc: 0.5481\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7183 - acc: 0.5516 - val_loss: 0.7161 - val_acc: 0.5533\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.7155 - acc: 0.5543 - val_loss: 0.7145 - val_acc: 0.5520\n",
      "\n",
      "Accuracy: 55.20%\n",
      "\n",
      "learning rate: 5.2e-05\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 48\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 275us/sample - loss: 1.8023 - acc: 0.5385 - val_loss: 0.6647 - val_acc: 0.6008\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7103 - acc: 0.5796 - val_loss: 0.6373 - val_acc: 0.6424\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6926 - acc: 0.5969 - val_loss: 0.9207 - val_acc: 0.5083\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6862 - acc: 0.6058 - val_loss: 0.6263 - val_acc: 0.6517\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6862 - acc: 0.6085 - val_loss: 0.6239 - val_acc: 0.6537\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6818 - acc: 0.6125 - val_loss: 0.6310 - val_acc: 0.6514\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6823 - acc: 0.6149 - val_loss: 0.8013 - val_acc: 0.5422\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6820 - acc: 0.6160 - val_loss: 0.7381 - val_acc: 0.5709\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6842 - acc: 0.6175 - val_loss: 0.6477 - val_acc: 0.6289\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6769 - acc: 0.6210 - val_loss: 0.7693 - val_acc: 0.5588\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6773 - acc: 0.6153 - val_loss: 0.6400 - val_acc: 0.6412\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6775 - acc: 0.6220 - val_loss: 0.6199 - val_acc: 0.6571\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6752 - acc: 0.6214 - val_loss: 0.6194 - val_acc: 0.6569\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6769 - acc: 0.6202 - val_loss: 0.9584 - val_acc: 0.5122\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6725 - acc: 0.6211 - val_loss: 0.6738 - val_acc: 0.6101\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6750 - acc: 0.6212 - val_loss: 0.8234 - val_acc: 0.5401\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6709 - acc: 0.6257 - val_loss: 0.6195 - val_acc: 0.6583\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6699 - acc: 0.6243 - val_loss: 0.6503 - val_acc: 0.6285\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6727 - acc: 0.6270 - val_loss: 0.6176 - val_acc: 0.6597\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6691 - acc: 0.6230 - val_loss: 0.6298 - val_acc: 0.6507\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6713 - acc: 0.6210 - val_loss: 0.9537 - val_acc: 0.5149\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6664 - acc: 0.6284 - val_loss: 0.6407 - val_acc: 0.6340\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6683 - acc: 0.6234 - val_loss: 0.6177 - val_acc: 0.6575\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6692 - acc: 0.6235 - val_loss: 0.6336 - val_acc: 0.6417\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6677 - acc: 0.6264 - val_loss: 0.6188 - val_acc: 0.6573\n",
      "\n",
      "Accuracy: 65.73%\n",
      "\n",
      "learning rate: 7.5e-05\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 2\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 1.3901 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 262us/sample - loss: 0.6932 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 262us/sample - loss: 0.6932 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6932 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6932 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6932 - acc: 0.4972 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 262us/sample - loss: 0.6932 - acc: 0.4976 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6932 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6932 - acc: 0.4964 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6932 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6932 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6932 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 262us/sample - loss: 0.6932 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6932 - acc: 0.4997 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6932 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 262us/sample - loss: 0.6932 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6932 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6932 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "\n",
      "Accuracy: 49.84%\n",
      "\n",
      "learning rate: 2.6e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 48\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 321us/sample - loss: 0.8681 - acc: 0.5393 - val_loss: 0.6817 - val_acc: 0.5615\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6917 - acc: 0.5680 - val_loss: 0.6830 - val_acc: 0.5628\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6833 - acc: 0.5821 - val_loss: 0.6822 - val_acc: 0.5590\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6723 - acc: 0.5975 - val_loss: 0.6359 - val_acc: 0.6460\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6661 - acc: 0.6045 - val_loss: 0.6339 - val_acc: 0.6451\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.6641 - acc: 0.6079 - val_loss: 0.6352 - val_acc: 0.6380\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6607 - acc: 0.6136 - val_loss: 0.6292 - val_acc: 0.6522\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6558 - acc: 0.6217 - val_loss: 0.6700 - val_acc: 0.5977\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6554 - acc: 0.6237 - val_loss: 0.6247 - val_acc: 0.6532\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6535 - acc: 0.6251 - val_loss: 0.6374 - val_acc: 0.6316\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6529 - acc: 0.6253 - val_loss: 0.6315 - val_acc: 0.6401\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6518 - acc: 0.6280 - val_loss: 0.6966 - val_acc: 0.5773\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6500 - acc: 0.6272 - val_loss: 0.6220 - val_acc: 0.6569\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6461 - acc: 0.6322 - val_loss: 0.6252 - val_acc: 0.6528\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6477 - acc: 0.6286 - val_loss: 0.6560 - val_acc: 0.6188\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6457 - acc: 0.6285 - val_loss: 0.6214 - val_acc: 0.6554\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6447 - acc: 0.6326 - val_loss: 0.7143 - val_acc: 0.5637\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6430 - acc: 0.6311 - val_loss: 0.6212 - val_acc: 0.6541\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6395 - acc: 0.6349 - val_loss: 0.6215 - val_acc: 0.6545\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6401 - acc: 0.6343 - val_loss: 0.6533 - val_acc: 0.6189\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6409 - acc: 0.6337 - val_loss: 0.6528 - val_acc: 0.6229\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6391 - acc: 0.6360 - val_loss: 0.6302 - val_acc: 0.6438\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6373 - acc: 0.6401 - val_loss: 0.6265 - val_acc: 0.6464\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6373 - acc: 0.6391 - val_loss: 0.6414 - val_acc: 0.6279\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 13s 316us/sample - loss: 0.6362 - acc: 0.6369 - val_loss: 0.6817 - val_acc: 0.6015\n",
      "\n",
      "Accuracy: 60.15%\n",
      "\n",
      "learning rate: 9.1e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 48\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.7981 - acc: 0.5220 - val_loss: 0.7391 - val_acc: 0.5113\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.7121 - acc: 0.5622 - val_loss: 0.6993 - val_acc: 0.5327\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6780 - acc: 0.5904 - val_loss: 0.6532 - val_acc: 0.6092\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6631 - acc: 0.6079 - val_loss: 0.6445 - val_acc: 0.6228\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6535 - acc: 0.6185 - val_loss: 0.6754 - val_acc: 0.5851\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6471 - acc: 0.6264 - val_loss: 0.6294 - val_acc: 0.6539\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.6408 - acc: 0.6313 - val_loss: 0.6281 - val_acc: 0.6466\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6388 - acc: 0.6344 - val_loss: 0.6273 - val_acc: 0.6489\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6346 - acc: 0.6405 - val_loss: 0.6290 - val_acc: 0.6427\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6327 - acc: 0.6416 - val_loss: 0.6167 - val_acc: 0.6581\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6332 - acc: 0.6402 - val_loss: 0.6331 - val_acc: 0.6395\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6328 - acc: 0.6396 - val_loss: 0.6172 - val_acc: 0.6565\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6312 - acc: 0.6455 - val_loss: 0.6199 - val_acc: 0.6574\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6301 - acc: 0.6422 - val_loss: 0.6223 - val_acc: 0.6483\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6303 - acc: 0.6432 - val_loss: 0.6207 - val_acc: 0.6588\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6285 - acc: 0.6449 - val_loss: 0.6409 - val_acc: 0.6319\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6286 - acc: 0.6469 - val_loss: 0.6532 - val_acc: 0.6220\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6282 - acc: 0.6484 - val_loss: 0.6318 - val_acc: 0.6380\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.6274 - acc: 0.6472 - val_loss: 0.6337 - val_acc: 0.6382\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6262 - acc: 0.6495 - val_loss: 0.6290 - val_acc: 0.6463\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6260 - acc: 0.6474 - val_loss: 0.6351 - val_acc: 0.6356\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6259 - acc: 0.6487 - val_loss: 0.6208 - val_acc: 0.6594\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6264 - acc: 0.6469 - val_loss: 0.6420 - val_acc: 0.6278\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6247 - acc: 0.6520 - val_loss: 0.6400 - val_acc: 0.6333\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6244 - acc: 0.6503 - val_loss: 0.6293 - val_acc: 0.6378\n",
      "\n",
      "Accuracy: 63.78%\n",
      "\n",
      "learning rate: 1.0e-06\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 2\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 12s 288us/sample - loss: 30.6869 - acc: 0.5004 - val_loss: 24.1217 - val_acc: 0.4984\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 17.7777 - acc: 0.5004 - val_loss: 11.8026 - val_acc: 0.4984\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 6.2180 - acc: 0.5004 - val_loss: 1.1547 - val_acc: 0.4984\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 281us/sample - loss: 0.7305 - acc: 0.4850 - val_loss: 0.6933 - val_acc: 0.5010\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 0.6933 - acc: 0.4992 - val_loss: 0.6932 - val_acc: 0.5017\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6933 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 283us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 281us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 279us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 275us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 279us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 281us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 279us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 281us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 279us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 281us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 278us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 283us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 278us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5015\n",
      "\n",
      "Accuracy: 50.15%\n",
      "\n",
      "learning rate: 4.9e-06\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 24\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 12s 291us/sample - loss: 2.2778 - acc: 0.5164 - val_loss: 0.7454 - val_acc: 0.5332\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.7356 - acc: 0.5331 - val_loss: 0.7188 - val_acc: 0.5442\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.7155 - acc: 0.5454 - val_loss: 0.7292 - val_acc: 0.5354\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 288us/sample - loss: 0.7016 - acc: 0.5573 - val_loss: 0.6900 - val_acc: 0.5702\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 288us/sample - loss: 0.6908 - acc: 0.5690 - val_loss: 0.6971 - val_acc: 0.5601\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 289us/sample - loss: 0.6830 - acc: 0.5765 - val_loss: 0.6763 - val_acc: 0.5844\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6753 - acc: 0.5880 - val_loss: 0.6674 - val_acc: 0.5967\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 283us/sample - loss: 0.6707 - acc: 0.5919 - val_loss: 0.6640 - val_acc: 0.6015\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6653 - acc: 0.5991 - val_loss: 0.6576 - val_acc: 0.6135\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 288us/sample - loss: 0.6606 - acc: 0.6091 - val_loss: 0.6532 - val_acc: 0.6166\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 289us/sample - loss: 0.6577 - acc: 0.6103 - val_loss: 0.6572 - val_acc: 0.6110\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6547 - acc: 0.6158 - val_loss: 0.6495 - val_acc: 0.6227\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6519 - acc: 0.6185 - val_loss: 0.6450 - val_acc: 0.6261\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6491 - acc: 0.6235 - val_loss: 0.6451 - val_acc: 0.6266\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6470 - acc: 0.6267 - val_loss: 0.6556 - val_acc: 0.6148\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6453 - acc: 0.6291 - val_loss: 0.6653 - val_acc: 0.6025\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 283us/sample - loss: 0.6447 - acc: 0.6300 - val_loss: 0.6393 - val_acc: 0.6363\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 287us/sample - loss: 0.6434 - acc: 0.6329 - val_loss: 0.6360 - val_acc: 0.6399\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6421 - acc: 0.6324 - val_loss: 0.6563 - val_acc: 0.6150\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6408 - acc: 0.6346 - val_loss: 0.6366 - val_acc: 0.6390\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6390 - acc: 0.6370 - val_loss: 0.6529 - val_acc: 0.6183\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6395 - acc: 0.6356 - val_loss: 0.6361 - val_acc: 0.6380\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 289us/sample - loss: 0.6389 - acc: 0.6372 - val_loss: 0.6470 - val_acc: 0.6283\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6371 - acc: 0.6366 - val_loss: 0.6529 - val_acc: 0.6215\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 287us/sample - loss: 0.6367 - acc: 0.6381 - val_loss: 0.6357 - val_acc: 0.6389\n",
      "\n",
      "Accuracy: 63.89%\n",
      "\n",
      "learning rate: 1.2e-04\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 40\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 275us/sample - loss: 0.9753 - acc: 0.5308 - val_loss: 1.1510 - val_acc: 0.5008\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.8510 - acc: 0.5711 - val_loss: 0.6762 - val_acc: 0.6227\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.8249 - acc: 0.5843 - val_loss: 0.7413 - val_acc: 0.5915\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.8061 - acc: 0.5889 - val_loss: 0.6288 - val_acc: 0.6560\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.8028 - acc: 0.5932 - val_loss: 0.8823 - val_acc: 0.5371\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.7976 - acc: 0.5946 - val_loss: 0.7200 - val_acc: 0.5943\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.7877 - acc: 0.5918 - val_loss: 0.6791 - val_acc: 0.6189\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.7743 - acc: 0.5959 - val_loss: 1.0999 - val_acc: 0.5074\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.7709 - acc: 0.5938 - val_loss: 0.6221 - val_acc: 0.6613\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.7468 - acc: 0.6018 - val_loss: 0.6393 - val_acc: 0.6434\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.7425 - acc: 0.6066 - val_loss: 0.6571 - val_acc: 0.6271\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.7391 - acc: 0.6006 - val_loss: 1.1999 - val_acc: 0.5011\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.7322 - acc: 0.6020 - val_loss: 1.1039 - val_acc: 0.5037\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.7270 - acc: 0.6015 - val_loss: 0.8850 - val_acc: 0.5281\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.7309 - acc: 0.6030 - val_loss: 0.8229 - val_acc: 0.5452\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.7212 - acc: 0.6032 - val_loss: 1.0459 - val_acc: 0.5062\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.7064 - acc: 0.6106 - val_loss: 0.6221 - val_acc: 0.6582\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.7112 - acc: 0.6050 - val_loss: 0.6225 - val_acc: 0.6588\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6996 - acc: 0.6100 - val_loss: 0.8286 - val_acc: 0.5398\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6875 - acc: 0.6159 - val_loss: 0.6375 - val_acc: 0.6369\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6892 - acc: 0.6129 - val_loss: 0.6821 - val_acc: 0.6059\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6845 - acc: 0.6174 - val_loss: 0.6594 - val_acc: 0.6252\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6783 - acc: 0.6206 - val_loss: 0.6191 - val_acc: 0.6558\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6739 - acc: 0.6222 - val_loss: 0.6299 - val_acc: 0.6483\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6766 - acc: 0.6208 - val_loss: 0.6598 - val_acc: 0.6249\n",
      "\n",
      "Accuracy: 62.49%\n",
      "\n",
      "learning rate: 6.3e-06\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 48\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 0.8534 - acc: 0.5272 - val_loss: 0.7090 - val_acc: 0.5447\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.7075 - acc: 0.5498 - val_loss: 0.6890 - val_acc: 0.5637\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6923 - acc: 0.5656 - val_loss: 0.6788 - val_acc: 0.5777\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 277us/sample - loss: 0.6839 - acc: 0.5733 - val_loss: 0.6909 - val_acc: 0.5702\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6744 - acc: 0.5884 - val_loss: 0.6665 - val_acc: 0.5965\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6704 - acc: 0.5921 - val_loss: 0.6601 - val_acc: 0.6065\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6646 - acc: 0.5992 - val_loss: 0.6811 - val_acc: 0.5785\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6603 - acc: 0.6056 - val_loss: 0.6558 - val_acc: 0.6131\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.6574 - acc: 0.6115 - val_loss: 0.6606 - val_acc: 0.6061\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6538 - acc: 0.6138 - val_loss: 0.6528 - val_acc: 0.6187\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6514 - acc: 0.6200 - val_loss: 0.6435 - val_acc: 0.6295\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6491 - acc: 0.6227 - val_loss: 0.6398 - val_acc: 0.6325\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6467 - acc: 0.6281 - val_loss: 0.6543 - val_acc: 0.6135\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6449 - acc: 0.6281 - val_loss: 0.6360 - val_acc: 0.6408\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6430 - acc: 0.6338 - val_loss: 0.6338 - val_acc: 0.6439\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.6418 - acc: 0.6332 - val_loss: 0.6366 - val_acc: 0.6376\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6406 - acc: 0.6339 - val_loss: 0.6314 - val_acc: 0.6445\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6396 - acc: 0.6370 - val_loss: 0.6325 - val_acc: 0.6432\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6385 - acc: 0.6371 - val_loss: 0.6302 - val_acc: 0.6461\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6365 - acc: 0.6415 - val_loss: 0.6307 - val_acc: 0.6446\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6367 - acc: 0.6399 - val_loss: 0.6424 - val_acc: 0.6339\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6363 - acc: 0.6397 - val_loss: 0.6276 - val_acc: 0.6485\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6364 - acc: 0.6406 - val_loss: 0.6266 - val_acc: 0.6490\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 275us/sample - loss: 0.6347 - acc: 0.6435 - val_loss: 0.6304 - val_acc: 0.6470\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6342 - acc: 0.6425 - val_loss: 0.6255 - val_acc: 0.6515\n",
      "\n",
      "Accuracy: 65.15%\n",
      "\n",
      "learning rate: 5.8e-06\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 48\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 1.6716 - acc: 0.5011 - val_loss: 0.7093 - val_acc: 0.5217\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.7102 - acc: 0.5257 - val_loss: 0.7043 - val_acc: 0.5329\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6983 - acc: 0.5444 - val_loss: 0.6824 - val_acc: 0.5654\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6889 - acc: 0.5575 - val_loss: 0.6876 - val_acc: 0.5609\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6798 - acc: 0.5730 - val_loss: 0.6668 - val_acc: 0.5970\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6747 - acc: 0.5852 - val_loss: 0.6718 - val_acc: 0.5852\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6691 - acc: 0.5944 - val_loss: 0.6639 - val_acc: 0.5997\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6650 - acc: 0.5996 - val_loss: 0.6516 - val_acc: 0.6253\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6613 - acc: 0.6041 - val_loss: 0.6486 - val_acc: 0.6249\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6580 - acc: 0.6117 - val_loss: 0.6462 - val_acc: 0.6261\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 0.6553 - acc: 0.6142 - val_loss: 0.6557 - val_acc: 0.6105\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6529 - acc: 0.6201 - val_loss: 0.6606 - val_acc: 0.6018\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 0.6517 - acc: 0.6210 - val_loss: 0.6696 - val_acc: 0.5925\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6488 - acc: 0.6237 - val_loss: 0.6402 - val_acc: 0.6360\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 0.6479 - acc: 0.6239 - val_loss: 0.6370 - val_acc: 0.6402\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 0.6473 - acc: 0.6269 - val_loss: 0.6385 - val_acc: 0.6328\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6463 - acc: 0.6262 - val_loss: 0.6713 - val_acc: 0.5921\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6454 - acc: 0.6274 - val_loss: 0.6398 - val_acc: 0.6398\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6428 - acc: 0.6285 - val_loss: 0.6407 - val_acc: 0.6314\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6434 - acc: 0.6310 - val_loss: 0.6338 - val_acc: 0.6418\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6430 - acc: 0.6317 - val_loss: 0.6371 - val_acc: 0.6403\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6419 - acc: 0.6315 - val_loss: 0.6407 - val_acc: 0.6342\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6410 - acc: 0.6327 - val_loss: 0.6414 - val_acc: 0.6338\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6403 - acc: 0.6325 - val_loss: 0.6382 - val_acc: 0.6379\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6397 - acc: 0.6367 - val_loss: 0.6400 - val_acc: 0.6355\n",
      "\n",
      "Accuracy: 63.55%\n",
      "\n",
      "learning rate: 3.8e-04\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 48\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.8659 - acc: 0.5270 - val_loss: 0.6837 - val_acc: 0.5139\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6976 - acc: 0.5065 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6932 - acc: 0.4969 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6932 - acc: 0.4951 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6932 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6932 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6932 - acc: 0.4965 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6932 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6932 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6932 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6932 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6932 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6932 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6932 - acc: 0.4978 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6932 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6932 - acc: 0.4966 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6932 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.6932 - acc: 0.4977 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6932 - acc: 0.4962 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 13s 316us/sample - loss: 0.6932 - acc: 0.4988 - val_loss: 0.6933 - val_acc: 0.4984\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6932 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6932 - acc: 0.4959 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6932 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "\n",
      "Accuracy: 49.84%\n",
      "\n",
      "learning rate: 8.8e-06\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 2\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 12s 290us/sample - loss: 0.6933 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6931 - acc: 0.4983 - val_loss: 0.6929 - val_acc: 0.4984\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6926 - acc: 0.5141 - val_loss: 0.6930 - val_acc: 0.4984\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6918 - acc: 0.5252 - val_loss: 0.6912 - val_acc: 0.5601\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6903 - acc: 0.5372 - val_loss: 0.6898 - val_acc: 0.5068\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6883 - acc: 0.5499 - val_loss: 0.6869 - val_acc: 0.5772\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6860 - acc: 0.5634 - val_loss: 0.6841 - val_acc: 0.5813\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6834 - acc: 0.5728 - val_loss: 0.6813 - val_acc: 0.5808\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6808 - acc: 0.5841 - val_loss: 0.6785 - val_acc: 0.5878\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6780 - acc: 0.5918 - val_loss: 0.6761 - val_acc: 0.5859\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6752 - acc: 0.6001 - val_loss: 0.6742 - val_acc: 0.6264\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6727 - acc: 0.6077 - val_loss: 0.6703 - val_acc: 0.6210\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 279us/sample - loss: 0.6701 - acc: 0.6116 - val_loss: 0.6675 - val_acc: 0.6245\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6675 - acc: 0.6174 - val_loss: 0.6658 - val_acc: 0.6332\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6651 - acc: 0.6228 - val_loss: 0.6636 - val_acc: 0.6219\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 283us/sample - loss: 0.6630 - acc: 0.6269 - val_loss: 0.6605 - val_acc: 0.6327\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6610 - acc: 0.6299 - val_loss: 0.6590 - val_acc: 0.6378\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6592 - acc: 0.6310 - val_loss: 0.6569 - val_acc: 0.6356\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6578 - acc: 0.6323 - val_loss: 0.6567 - val_acc: 0.6372\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6559 - acc: 0.6348 - val_loss: 0.6539 - val_acc: 0.6448\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 283us/sample - loss: 0.6547 - acc: 0.6349 - val_loss: 0.6522 - val_acc: 0.6445\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 283us/sample - loss: 0.6532 - acc: 0.6385 - val_loss: 0.6555 - val_acc: 0.6264\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 283us/sample - loss: 0.6517 - acc: 0.6395 - val_loss: 0.6515 - val_acc: 0.6453\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6504 - acc: 0.6411 - val_loss: 0.6504 - val_acc: 0.6467\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6493 - acc: 0.6419 - val_loss: 0.6469 - val_acc: 0.6468\n",
      "\n",
      "Accuracy: 64.68%\n",
      "\n",
      "learning rate: 7.6e-05\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 48\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.8448 - acc: 0.5364 - val_loss: 0.6598 - val_acc: 0.6144\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.7681 - acc: 0.5822 - val_loss: 0.7647 - val_acc: 0.5634\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.7613 - acc: 0.5950 - val_loss: 0.6912 - val_acc: 0.6088\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.7521 - acc: 0.5979 - val_loss: 0.9647 - val_acc: 0.5208\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7436 - acc: 0.6023 - val_loss: 0.7239 - val_acc: 0.5881\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7410 - acc: 0.6022 - val_loss: 0.6620 - val_acc: 0.6249\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.7469 - acc: 0.6007 - val_loss: 0.8038 - val_acc: 0.5563\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.7375 - acc: 0.6012 - val_loss: 0.7329 - val_acc: 0.5839\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.7354 - acc: 0.6061 - val_loss: 0.6876 - val_acc: 0.6072\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.7362 - acc: 0.6009 - val_loss: 0.8415 - val_acc: 0.5416\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 275us/sample - loss: 0.7364 - acc: 0.6041 - val_loss: 0.9698 - val_acc: 0.5170\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.7260 - acc: 0.6094 - val_loss: 0.8093 - val_acc: 0.5539\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.7251 - acc: 0.6079 - val_loss: 0.6817 - val_acc: 0.6125\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.7175 - acc: 0.6096 - val_loss: 0.6553 - val_acc: 0.6296\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.7101 - acc: 0.6126 - val_loss: 0.6197 - val_acc: 0.6599\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.7153 - acc: 0.6047 - val_loss: 0.6513 - val_acc: 0.6309\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7052 - acc: 0.6142 - val_loss: 0.7404 - val_acc: 0.5789\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.7026 - acc: 0.6141 - val_loss: 1.0002 - val_acc: 0.5085\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.7047 - acc: 0.6116 - val_loss: 0.6919 - val_acc: 0.6049\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.7010 - acc: 0.6149 - val_loss: 0.6199 - val_acc: 0.6560\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.7013 - acc: 0.6104 - val_loss: 0.6515 - val_acc: 0.6275\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6970 - acc: 0.6156 - val_loss: 0.6567 - val_acc: 0.6247\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6939 - acc: 0.6142 - val_loss: 0.6256 - val_acc: 0.6529\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6897 - acc: 0.6202 - val_loss: 0.6704 - val_acc: 0.6141\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6899 - acc: 0.6180 - val_loss: 0.7175 - val_acc: 0.5884\n",
      "\n",
      "Accuracy: 58.84%\n",
      "\n",
      "learning rate: 9.3e-06\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 48\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 281us/sample - loss: 5.9956 - acc: 0.5160 - val_loss: 0.7592 - val_acc: 0.5381\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.7264 - acc: 0.5526 - val_loss: 0.7421 - val_acc: 0.5430\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6974 - acc: 0.5789 - val_loss: 0.7022 - val_acc: 0.5670\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6834 - acc: 0.5917 - val_loss: 0.6787 - val_acc: 0.5932\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 275us/sample - loss: 0.6734 - acc: 0.6015 - val_loss: 0.7033 - val_acc: 0.5754\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6667 - acc: 0.6103 - val_loss: 0.6465 - val_acc: 0.6261\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6618 - acc: 0.6130 - val_loss: 0.6628 - val_acc: 0.6121\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6601 - acc: 0.6155 - val_loss: 0.6362 - val_acc: 0.6407\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6558 - acc: 0.6221 - val_loss: 0.6592 - val_acc: 0.6165\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6576 - acc: 0.6220 - val_loss: 0.6407 - val_acc: 0.6370\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.6530 - acc: 0.6265 - val_loss: 0.6988 - val_acc: 0.5853\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6502 - acc: 0.6297 - val_loss: 0.6324 - val_acc: 0.6464\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6526 - acc: 0.6272 - val_loss: 0.6770 - val_acc: 0.6030\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6492 - acc: 0.6312 - val_loss: 0.6941 - val_acc: 0.5881\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6480 - acc: 0.6335 - val_loss: 0.6431 - val_acc: 0.6385\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6473 - acc: 0.6334 - val_loss: 0.6707 - val_acc: 0.6142\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6475 - acc: 0.6336 - val_loss: 0.6343 - val_acc: 0.6434\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6468 - acc: 0.6336 - val_loss: 0.6281 - val_acc: 0.6521\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6497 - acc: 0.6326 - val_loss: 0.6417 - val_acc: 0.6382\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6439 - acc: 0.6362 - val_loss: 0.6370 - val_acc: 0.6424\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6471 - acc: 0.6375 - val_loss: 0.6243 - val_acc: 0.6548\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6456 - acc: 0.6367 - val_loss: 0.6545 - val_acc: 0.6286\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6458 - acc: 0.6367 - val_loss: 0.6219 - val_acc: 0.6569\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6447 - acc: 0.6389 - val_loss: 0.6435 - val_acc: 0.6373\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6436 - acc: 0.6353 - val_loss: 0.8561 - val_acc: 0.5262\n",
      "\n",
      "Accuracy: 52.62%\n",
      "\n",
      "learning rate: 1.5e-05\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 2\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 1.0139 - acc: 0.5418 - val_loss: 0.6812 - val_acc: 0.5759\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6695 - acc: 0.5929 - val_loss: 0.6692 - val_acc: 0.5857\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6549 - acc: 0.6164 - val_loss: 0.6438 - val_acc: 0.6320\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6480 - acc: 0.6257 - val_loss: 0.6984 - val_acc: 0.5670\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6442 - acc: 0.6337 - val_loss: 0.6763 - val_acc: 0.5902\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6417 - acc: 0.6343 - val_loss: 0.6665 - val_acc: 0.6044\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6405 - acc: 0.6373 - val_loss: 0.6279 - val_acc: 0.6506\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6378 - acc: 0.6393 - val_loss: 0.6252 - val_acc: 0.6549\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6381 - acc: 0.6407 - val_loss: 0.6229 - val_acc: 0.6545\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6364 - acc: 0.6438 - val_loss: 0.6284 - val_acc: 0.6489\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6357 - acc: 0.6445 - val_loss: 0.6293 - val_acc: 0.6512\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6356 - acc: 0.6427 - val_loss: 0.6428 - val_acc: 0.6340\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6358 - acc: 0.6431 - val_loss: 0.6291 - val_acc: 0.6462\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6353 - acc: 0.6438 - val_loss: 0.6380 - val_acc: 0.6403\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6348 - acc: 0.6465 - val_loss: 0.6272 - val_acc: 0.6491\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6351 - acc: 0.6467 - val_loss: 0.6261 - val_acc: 0.6503\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6347 - acc: 0.6446 - val_loss: 0.6197 - val_acc: 0.6613\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6339 - acc: 0.6466 - val_loss: 0.6379 - val_acc: 0.6370\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6327 - acc: 0.6484 - val_loss: 0.6210 - val_acc: 0.6574\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6336 - acc: 0.6491 - val_loss: 0.6294 - val_acc: 0.6494\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6346 - acc: 0.6485 - val_loss: 0.6275 - val_acc: 0.6485\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6337 - acc: 0.6453 - val_loss: 0.6413 - val_acc: 0.6373\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6331 - acc: 0.6493 - val_loss: 0.6209 - val_acc: 0.6593\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6325 - acc: 0.6479 - val_loss: 0.6191 - val_acc: 0.6588\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6333 - acc: 0.6455 - val_loss: 0.6191 - val_acc: 0.6601\n",
      "\n",
      "Accuracy: 66.01%\n",
      "\n",
      "learning rate: 4.5e-06\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 48\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 277us/sample - loss: 0.7487 - acc: 0.5461 - val_loss: 0.7214 - val_acc: 0.5599\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.7166 - acc: 0.5580 - val_loss: 0.7017 - val_acc: 0.5700\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.7050 - acc: 0.5687 - val_loss: 0.6981 - val_acc: 0.5728\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6965 - acc: 0.5754 - val_loss: 0.6903 - val_acc: 0.5760\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6890 - acc: 0.5810 - val_loss: 0.6830 - val_acc: 0.5840\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6830 - acc: 0.5841 - val_loss: 0.6726 - val_acc: 0.5949\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6776 - acc: 0.5909 - val_loss: 0.6860 - val_acc: 0.5893\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6725 - acc: 0.5961 - val_loss: 0.6650 - val_acc: 0.6029\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6683 - acc: 0.6018 - val_loss: 0.6596 - val_acc: 0.6080\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6647 - acc: 0.6052 - val_loss: 0.6538 - val_acc: 0.6158\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6607 - acc: 0.6098 - val_loss: 0.6524 - val_acc: 0.6228\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6583 - acc: 0.6120 - val_loss: 0.6480 - val_acc: 0.6207\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.6546 - acc: 0.6189 - val_loss: 0.6520 - val_acc: 0.6215\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6527 - acc: 0.6204 - val_loss: 0.6434 - val_acc: 0.6306\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6508 - acc: 0.6241 - val_loss: 0.6409 - val_acc: 0.6324\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6481 - acc: 0.6279 - val_loss: 0.6679 - val_acc: 0.6038\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6459 - acc: 0.6276 - val_loss: 0.6395 - val_acc: 0.6328\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6455 - acc: 0.6298 - val_loss: 0.6357 - val_acc: 0.6378\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6433 - acc: 0.6316 - val_loss: 0.6343 - val_acc: 0.6399\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6424 - acc: 0.6340 - val_loss: 0.6472 - val_acc: 0.6288\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6411 - acc: 0.6339 - val_loss: 0.6344 - val_acc: 0.6418\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6401 - acc: 0.6363 - val_loss: 0.6355 - val_acc: 0.6418\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6391 - acc: 0.6380 - val_loss: 0.6499 - val_acc: 0.6253\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6376 - acc: 0.6385 - val_loss: 0.6295 - val_acc: 0.6468\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.6372 - acc: 0.6401 - val_loss: 0.6327 - val_acc: 0.6435\n",
      "\n",
      "Accuracy: 64.35%\n",
      "\n",
      "learning rate: 4.3e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 48\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 315us/sample - loss: 0.8734 - acc: 0.5179 - val_loss: 0.6983 - val_acc: 0.5517\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.7428 - acc: 0.5554 - val_loss: 0.7074 - val_acc: 0.5445\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.7234 - acc: 0.5755 - val_loss: 0.8557 - val_acc: 0.5011\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.7100 - acc: 0.5856 - val_loss: 0.6349 - val_acc: 0.6434\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6945 - acc: 0.5942 - val_loss: 0.6288 - val_acc: 0.6505\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6911 - acc: 0.5984 - val_loss: 0.6318 - val_acc: 0.6453\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6798 - acc: 0.6053 - val_loss: 0.7075 - val_acc: 0.5705\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6770 - acc: 0.6088 - val_loss: 0.6264 - val_acc: 0.6531\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6705 - acc: 0.6135 - val_loss: 0.6287 - val_acc: 0.6516\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6643 - acc: 0.6173 - val_loss: 0.6268 - val_acc: 0.6518\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6631 - acc: 0.6162 - val_loss: 0.7597 - val_acc: 0.5374\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6580 - acc: 0.6220 - val_loss: 0.6272 - val_acc: 0.6510\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6532 - acc: 0.6251 - val_loss: 0.6333 - val_acc: 0.6409\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6495 - acc: 0.6257 - val_loss: 0.6840 - val_acc: 0.5833\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6459 - acc: 0.6300 - val_loss: 0.6292 - val_acc: 0.6456\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6452 - acc: 0.6296 - val_loss: 0.6227 - val_acc: 0.6553\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6441 - acc: 0.6313 - val_loss: 0.6263 - val_acc: 0.6588\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6417 - acc: 0.6348 - val_loss: 0.6273 - val_acc: 0.6526\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6394 - acc: 0.6341 - val_loss: 0.6216 - val_acc: 0.6545\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6357 - acc: 0.6384 - val_loss: 0.6316 - val_acc: 0.6440\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6347 - acc: 0.6399 - val_loss: 0.6318 - val_acc: 0.6408\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6330 - acc: 0.6411 - val_loss: 0.6494 - val_acc: 0.6270\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6329 - acc: 0.6436 - val_loss: 0.6666 - val_acc: 0.6082\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6323 - acc: 0.6450 - val_loss: 0.6233 - val_acc: 0.6522\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 0.6309 - acc: 0.6455 - val_loss: 0.6215 - val_acc: 0.6528\n",
      "\n",
      "Accuracy: 65.28%\n",
      "\n",
      "learning rate: 2.3e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 2\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 12s 288us/sample - loss: 0.7036 - acc: 0.5004 - val_loss: 0.6955 - val_acc: 0.4984\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 0.6938 - acc: 0.5004 - val_loss: 0.6933 - val_acc: 0.4984\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 279us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 0.6932 - acc: 0.4959 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 0.6932 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 279us/sample - loss: 0.6932 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 279us/sample - loss: 0.6932 - acc: 0.4957 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6932 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6932 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 279us/sample - loss: 0.6932 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 278us/sample - loss: 0.6932 - acc: 0.4950 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 279us/sample - loss: 0.6932 - acc: 0.5010 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 0.6932 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 0.6932 - acc: 0.4978 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 279us/sample - loss: 0.6932 - acc: 0.4971 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 281us/sample - loss: 0.6932 - acc: 0.4973 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 277us/sample - loss: 0.6932 - acc: 0.4934 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 281us/sample - loss: 0.6932 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 281us/sample - loss: 0.6932 - acc: 0.4973 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 281us/sample - loss: 0.6932 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 279us/sample - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "\n",
      "Accuracy: 50.16%\n",
      "\n",
      "learning rate: 1.2e-05\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 2\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 15.2176 - acc: 0.4978 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6931 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6931 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6931 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6931 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6931 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6931 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6931 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6931 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6931 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6931 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6931 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "\n",
      "Accuracy: 49.84%\n",
      "\n",
      "learning rate: 4.0e-06\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 2\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 12s 292us/sample - loss: 0.7001 - acc: 0.4996 - val_loss: 0.6983 - val_acc: 0.5016\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6976 - acc: 0.4996 - val_loss: 0.6963 - val_acc: 0.5016\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6959 - acc: 0.4996 - val_loss: 0.6951 - val_acc: 0.5016\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6949 - acc: 0.4996 - val_loss: 0.6943 - val_acc: 0.5016\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6942 - acc: 0.4996 - val_loss: 0.6938 - val_acc: 0.5016\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6938 - acc: 0.4996 - val_loss: 0.6935 - val_acc: 0.5016\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6936 - acc: 0.4996 - val_loss: 0.6934 - val_acc: 0.5016\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6934 - acc: 0.4996 - val_loss: 0.6933 - val_acc: 0.5016\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6933 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 285us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 282us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 280us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 283us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6932 - acc: 0.4960 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 287us/sample - loss: 0.6932 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 287us/sample - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6931 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 286us/sample - loss: 0.6931 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 288us/sample - loss: 0.6931 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 284us/sample - loss: 0.6931 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.4984\n",
      "\n",
      "Accuracy: 49.84%\n",
      "\n",
      "learning rate: 1.8e-05\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 33\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 1.6327 - acc: 0.4514 - val_loss: 1.0184 - val_acc: 0.4404\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.9489 - acc: 0.4695 - val_loss: 0.8649 - val_acc: 0.4771\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.8308 - acc: 0.5063 - val_loss: 0.7760 - val_acc: 0.5234\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.7652 - acc: 0.5399 - val_loss: 0.7456 - val_acc: 0.5496\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.7332 - acc: 0.5668 - val_loss: 0.6931 - val_acc: 0.5959\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.7097 - acc: 0.5877 - val_loss: 0.7056 - val_acc: 0.5860\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6964 - acc: 0.5981 - val_loss: 0.6895 - val_acc: 0.6057\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6877 - acc: 0.6108 - val_loss: 0.7419 - val_acc: 0.5760\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6811 - acc: 0.6163 - val_loss: 0.7497 - val_acc: 0.5733\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6785 - acc: 0.6193 - val_loss: 0.6513 - val_acc: 0.6334\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6718 - acc: 0.6243 - val_loss: 0.7463 - val_acc: 0.5758\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6696 - acc: 0.6249 - val_loss: 0.6502 - val_acc: 0.6388\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6693 - acc: 0.6256 - val_loss: 0.6386 - val_acc: 0.6450\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 262us/sample - loss: 0.6650 - acc: 0.6276 - val_loss: 0.6497 - val_acc: 0.6332\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6673 - acc: 0.6270 - val_loss: 0.6375 - val_acc: 0.6472\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6625 - acc: 0.6291 - val_loss: 0.6489 - val_acc: 0.6409\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6637 - acc: 0.6292 - val_loss: 0.6320 - val_acc: 0.6504\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6610 - acc: 0.6295 - val_loss: 0.6322 - val_acc: 0.6498\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6621 - acc: 0.6301 - val_loss: 0.6380 - val_acc: 0.6424\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6601 - acc: 0.6317 - val_loss: 0.6330 - val_acc: 0.6484\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6598 - acc: 0.6293 - val_loss: 0.6843 - val_acc: 0.6098\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6598 - acc: 0.6297 - val_loss: 0.7452 - val_acc: 0.5750\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6588 - acc: 0.6298 - val_loss: 0.6400 - val_acc: 0.6413\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6596 - acc: 0.6306 - val_loss: 0.6271 - val_acc: 0.6533\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6593 - acc: 0.6305 - val_loss: 0.6676 - val_acc: 0.6195\n",
      "\n",
      "Accuracy: 61.95%\n",
      "\n",
      "learning rate: 1.8e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 34\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 0.8864 - acc: 0.4977 - val_loss: 0.7169 - val_acc: 0.5183\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.7285 - acc: 0.5282 - val_loss: 0.6848 - val_acc: 0.5650\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.7005 - acc: 0.5521 - val_loss: 0.7247 - val_acc: 0.5186\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 13s 315us/sample - loss: 0.6847 - acc: 0.5752 - val_loss: 0.6576 - val_acc: 0.6120\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6756 - acc: 0.5908 - val_loss: 0.6586 - val_acc: 0.6024\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6686 - acc: 0.5982 - val_loss: 0.6594 - val_acc: 0.6042\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6623 - acc: 0.6041 - val_loss: 0.6778 - val_acc: 0.5766\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6610 - acc: 0.6120 - val_loss: 0.6778 - val_acc: 0.5817\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6578 - acc: 0.6174 - val_loss: 0.6630 - val_acc: 0.6020\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6579 - acc: 0.6133 - val_loss: 0.6286 - val_acc: 0.6524\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.6524 - acc: 0.6224 - val_loss: 0.6667 - val_acc: 0.6050\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6522 - acc: 0.6240 - val_loss: 0.6264 - val_acc: 0.6528\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6511 - acc: 0.6276 - val_loss: 0.6538 - val_acc: 0.6166\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6507 - acc: 0.6222 - val_loss: 0.6455 - val_acc: 0.6289\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6488 - acc: 0.6277 - val_loss: 0.6296 - val_acc: 0.6446\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6468 - acc: 0.6321 - val_loss: 0.6235 - val_acc: 0.6582\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6490 - acc: 0.6286 - val_loss: 0.6236 - val_acc: 0.6573\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6464 - acc: 0.6315 - val_loss: 0.6911 - val_acc: 0.5900\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6469 - acc: 0.6279 - val_loss: 0.6593 - val_acc: 0.6183\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6438 - acc: 0.6321 - val_loss: 0.7295 - val_acc: 0.5592\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6461 - acc: 0.6319 - val_loss: 0.6350 - val_acc: 0.6388\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.6447 - acc: 0.6332 - val_loss: 0.6620 - val_acc: 0.6140\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6442 - acc: 0.6338 - val_loss: 0.6454 - val_acc: 0.6264\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6441 - acc: 0.6348 - val_loss: 0.6227 - val_acc: 0.6571\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6406 - acc: 0.6389 - val_loss: 0.6365 - val_acc: 0.6392\n",
      "\n",
      "Accuracy: 63.92%\n",
      "\n",
      "learning rate: 1.8e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 34\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.7368 - acc: 0.5377 - val_loss: 0.9151 - val_acc: 0.5015\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6961 - acc: 0.5687 - val_loss: 0.7810 - val_acc: 0.5063\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6862 - acc: 0.5814 - val_loss: 0.7256 - val_acc: 0.5275\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6793 - acc: 0.5903 - val_loss: 0.6386 - val_acc: 0.6371\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6740 - acc: 0.6007 - val_loss: 0.6615 - val_acc: 0.5992\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6689 - acc: 0.6061 - val_loss: 0.6468 - val_acc: 0.6225\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6658 - acc: 0.6095 - val_loss: 0.6897 - val_acc: 0.5796\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6646 - acc: 0.6114 - val_loss: 0.6540 - val_acc: 0.6149\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6648 - acc: 0.6147 - val_loss: 0.6665 - val_acc: 0.5974\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6649 - acc: 0.6152 - val_loss: 0.6248 - val_acc: 0.6499\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6629 - acc: 0.6191 - val_loss: 0.6341 - val_acc: 0.6405\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6606 - acc: 0.6188 - val_loss: 0.6406 - val_acc: 0.6305\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.6594 - acc: 0.6209 - val_loss: 0.6231 - val_acc: 0.6518\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6579 - acc: 0.6223 - val_loss: 1.0389 - val_acc: 0.5028\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6573 - acc: 0.6241 - val_loss: 0.7093 - val_acc: 0.5740\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6596 - acc: 0.6226 - val_loss: 0.6274 - val_acc: 0.6464\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6561 - acc: 0.6247 - val_loss: 0.6338 - val_acc: 0.6392\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6586 - acc: 0.6246 - val_loss: 0.6725 - val_acc: 0.5988\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6555 - acc: 0.6265 - val_loss: 0.6900 - val_acc: 0.5895\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6543 - acc: 0.6251 - val_loss: 0.6360 - val_acc: 0.6366\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6549 - acc: 0.6260 - val_loss: 0.6384 - val_acc: 0.6389\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6547 - acc: 0.6255 - val_loss: 0.7479 - val_acc: 0.5504\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6551 - acc: 0.6248 - val_loss: 0.6619 - val_acc: 0.6122\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6501 - acc: 0.6274 - val_loss: 0.7071 - val_acc: 0.5761\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6543 - acc: 0.6295 - val_loss: 0.6200 - val_acc: 0.6547\n",
      "\n",
      "Accuracy: 65.47%\n",
      "\n",
      "learning rate: 1.8e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 33\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 0.7709 - acc: 0.4861 - val_loss: 0.7610 - val_acc: 0.5028\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.7129 - acc: 0.5204 - val_loss: 0.6828 - val_acc: 0.5602\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6956 - acc: 0.5481 - val_loss: 0.6700 - val_acc: 0.5894\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6821 - acc: 0.5706 - val_loss: 0.6991 - val_acc: 0.5306\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6707 - acc: 0.5886 - val_loss: 0.6950 - val_acc: 0.5382\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6647 - acc: 0.6022 - val_loss: 0.6433 - val_acc: 0.6378\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6594 - acc: 0.6087 - val_loss: 0.6531 - val_acc: 0.6073\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6525 - acc: 0.6164 - val_loss: 0.6337 - val_acc: 0.6425\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6511 - acc: 0.6190 - val_loss: 0.6333 - val_acc: 0.6399\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6491 - acc: 0.6250 - val_loss: 0.6455 - val_acc: 0.6286\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6474 - acc: 0.6278 - val_loss: 0.6293 - val_acc: 0.6466\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6432 - acc: 0.6308 - val_loss: 0.6406 - val_acc: 0.6340\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6446 - acc: 0.6323 - val_loss: 0.6508 - val_acc: 0.6198\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6417 - acc: 0.6343 - val_loss: 0.6323 - val_acc: 0.6406\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6408 - acc: 0.6364 - val_loss: 0.6497 - val_acc: 0.6243\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6405 - acc: 0.6357 - val_loss: 0.6299 - val_acc: 0.6407\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6418 - acc: 0.6364 - val_loss: 0.6214 - val_acc: 0.6554\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6403 - acc: 0.6353 - val_loss: 0.6445 - val_acc: 0.6314\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6380 - acc: 0.6409 - val_loss: 0.6208 - val_acc: 0.6558\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6374 - acc: 0.6392 - val_loss: 0.6287 - val_acc: 0.6472\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6371 - acc: 0.6403 - val_loss: 0.6408 - val_acc: 0.6352\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6373 - acc: 0.6388 - val_loss: 0.6218 - val_acc: 0.6582\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6361 - acc: 0.6416 - val_loss: 0.6199 - val_acc: 0.6572\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6367 - acc: 0.6396 - val_loss: 0.6218 - val_acc: 0.6543\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6355 - acc: 0.6436 - val_loss: 0.6190 - val_acc: 0.6591\n",
      "\n",
      "Accuracy: 65.91%\n",
      "\n",
      "learning rate: 1.9e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 32\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 318us/sample - loss: 1.0587 - acc: 0.5168 - val_loss: 0.7084 - val_acc: 0.5435\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.7160 - acc: 0.5488 - val_loss: 0.6769 - val_acc: 0.5815\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6940 - acc: 0.5692 - val_loss: 0.6842 - val_acc: 0.5757\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6798 - acc: 0.5885 - val_loss: 0.6855 - val_acc: 0.5759\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6690 - acc: 0.6026 - val_loss: 0.6603 - val_acc: 0.6127\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6622 - acc: 0.6123 - val_loss: 0.6341 - val_acc: 0.6418\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6588 - acc: 0.6167 - val_loss: 0.6358 - val_acc: 0.6378\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6551 - acc: 0.6230 - val_loss: 0.6445 - val_acc: 0.6309\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6504 - acc: 0.6280 - val_loss: 0.6400 - val_acc: 0.6305\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6500 - acc: 0.6274 - val_loss: 0.7291 - val_acc: 0.5605\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6499 - acc: 0.6286 - val_loss: 0.6269 - val_acc: 0.6508\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6478 - acc: 0.6293 - val_loss: 0.6419 - val_acc: 0.6318\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6489 - acc: 0.6283 - val_loss: 0.6597 - val_acc: 0.6120\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6463 - acc: 0.6316 - val_loss: 0.6224 - val_acc: 0.6541\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6475 - acc: 0.6291 - val_loss: 0.6392 - val_acc: 0.6361\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6456 - acc: 0.6314 - val_loss: 0.6218 - val_acc: 0.6530\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6450 - acc: 0.6345 - val_loss: 0.6207 - val_acc: 0.6552\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6429 - acc: 0.6378 - val_loss: 0.6205 - val_acc: 0.6576\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6429 - acc: 0.6354 - val_loss: 0.6289 - val_acc: 0.6478\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6410 - acc: 0.6363 - val_loss: 0.6212 - val_acc: 0.6544\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6418 - acc: 0.6320 - val_loss: 0.6254 - val_acc: 0.6515\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6392 - acc: 0.6331 - val_loss: 0.6294 - val_acc: 0.6419\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6401 - acc: 0.6349 - val_loss: 0.6469 - val_acc: 0.6248\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6404 - acc: 0.6330 - val_loss: 0.6212 - val_acc: 0.6580\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6384 - acc: 0.6389 - val_loss: 0.6219 - val_acc: 0.6533\n",
      "\n",
      "Accuracy: 65.33%\n",
      "\n",
      "learning rate: 1.9e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 32\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 315us/sample - loss: 4.3789 - acc: 0.5776 - val_loss: 0.6507 - val_acc: 0.6192\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6698 - acc: 0.5944 - val_loss: 0.6491 - val_acc: 0.6220\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6634 - acc: 0.6040 - val_loss: 0.6425 - val_acc: 0.6329\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6585 - acc: 0.6109 - val_loss: 0.6391 - val_acc: 0.6356\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6569 - acc: 0.6104 - val_loss: 0.6396 - val_acc: 0.6322\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6541 - acc: 0.6183 - val_loss: 0.6521 - val_acc: 0.6165\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6515 - acc: 0.6201 - val_loss: 0.6855 - val_acc: 0.5752\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6495 - acc: 0.6234 - val_loss: 0.6378 - val_acc: 0.6332\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6484 - acc: 0.6224 - val_loss: 0.6352 - val_acc: 0.6391\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6485 - acc: 0.6236 - val_loss: 0.6299 - val_acc: 0.6446\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6464 - acc: 0.6274 - val_loss: 0.6283 - val_acc: 0.6454\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6463 - acc: 0.6244 - val_loss: 0.6337 - val_acc: 0.6405\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6445 - acc: 0.6297 - val_loss: 0.6283 - val_acc: 0.6468\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6434 - acc: 0.6314 - val_loss: 0.6318 - val_acc: 0.6448\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6428 - acc: 0.6303 - val_loss: 0.7069 - val_acc: 0.5763\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6430 - acc: 0.6303 - val_loss: 0.6491 - val_acc: 0.6239\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6425 - acc: 0.6306 - val_loss: 0.6552 - val_acc: 0.6159\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6407 - acc: 0.6328 - val_loss: 0.6608 - val_acc: 0.6053\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6417 - acc: 0.6297 - val_loss: 0.6252 - val_acc: 0.6507\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6398 - acc: 0.6358 - val_loss: 0.6421 - val_acc: 0.6328\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6385 - acc: 0.6358 - val_loss: 0.6238 - val_acc: 0.6544\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6377 - acc: 0.6404 - val_loss: 0.6563 - val_acc: 0.6177\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6385 - acc: 0.6359 - val_loss: 0.6251 - val_acc: 0.6496\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6375 - acc: 0.6380 - val_loss: 0.6417 - val_acc: 0.6321\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6386 - acc: 0.6372 - val_loss: 0.6230 - val_acc: 0.6519\n",
      "\n",
      "Accuracy: 65.19%\n",
      "\n",
      "learning rate: 1.9e-05\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 32\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.9799 - acc: 0.4911 - val_loss: 0.8289 - val_acc: 0.5070\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.7739 - acc: 0.5299 - val_loss: 0.7747 - val_acc: 0.5275\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7192 - acc: 0.5667 - val_loss: 0.6703 - val_acc: 0.6009\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6938 - acc: 0.5888 - val_loss: 0.6509 - val_acc: 0.6233\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6787 - acc: 0.6021 - val_loss: 0.6562 - val_acc: 0.6177\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6673 - acc: 0.6153 - val_loss: 0.6895 - val_acc: 0.5866\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6638 - acc: 0.6191 - val_loss: 0.6299 - val_acc: 0.6484\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6641 - acc: 0.6192 - val_loss: 0.6274 - val_acc: 0.6520\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6573 - acc: 0.6283 - val_loss: 0.6774 - val_acc: 0.6021\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6602 - acc: 0.6243 - val_loss: 0.6354 - val_acc: 0.6402\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6596 - acc: 0.6245 - val_loss: 0.6580 - val_acc: 0.6218\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6537 - acc: 0.6307 - val_loss: 0.6221 - val_acc: 0.6561\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6545 - acc: 0.6281 - val_loss: 0.6300 - val_acc: 0.6490\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6570 - acc: 0.6286 - val_loss: 0.6335 - val_acc: 0.6431\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6549 - acc: 0.6282 - val_loss: 0.6597 - val_acc: 0.6194\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6571 - acc: 0.6282 - val_loss: 0.6394 - val_acc: 0.6358\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6554 - acc: 0.6329 - val_loss: 0.7251 - val_acc: 0.5738\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6547 - acc: 0.6299 - val_loss: 0.6200 - val_acc: 0.6593\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6520 - acc: 0.6333 - val_loss: 0.7468 - val_acc: 0.5604\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6521 - acc: 0.6327 - val_loss: 0.6369 - val_acc: 0.6418\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6507 - acc: 0.6318 - val_loss: 0.6366 - val_acc: 0.6423\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6513 - acc: 0.6318 - val_loss: 0.6258 - val_acc: 0.6528\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6524 - acc: 0.6319 - val_loss: 0.6338 - val_acc: 0.6447\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6539 - acc: 0.6301 - val_loss: 0.6290 - val_acc: 0.6500\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6528 - acc: 0.6295 - val_loss: 0.6518 - val_acc: 0.6258\n",
      "\n",
      "Accuracy: 62.58%\n",
      "\n",
      "learning rate: 6.9e-06\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 31\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 319us/sample - loss: 1.2406 - acc: 0.5112 - val_loss: 0.8865 - val_acc: 0.5203\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.8830 - acc: 0.5164 - val_loss: 0.8462 - val_acc: 0.5240\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.8375 - acc: 0.5249 - val_loss: 0.8026 - val_acc: 0.5336\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.8002 - acc: 0.5326 - val_loss: 0.7613 - val_acc: 0.5437\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.7693 - acc: 0.5439 - val_loss: 0.7580 - val_acc: 0.5509\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.7456 - acc: 0.5525 - val_loss: 0.7284 - val_acc: 0.5604\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.7278 - acc: 0.5622 - val_loss: 0.7048 - val_acc: 0.5754\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.7124 - acc: 0.5695 - val_loss: 0.6855 - val_acc: 0.5889\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6993 - acc: 0.5797 - val_loss: 0.7044 - val_acc: 0.5770\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6887 - acc: 0.5862 - val_loss: 0.6698 - val_acc: 0.6002\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6808 - acc: 0.5952 - val_loss: 0.6774 - val_acc: 0.5952\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6759 - acc: 0.5985 - val_loss: 0.7020 - val_acc: 0.5766\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6694 - acc: 0.6040 - val_loss: 0.6535 - val_acc: 0.6221\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6646 - acc: 0.6102 - val_loss: 0.6489 - val_acc: 0.6274\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6617 - acc: 0.6143 - val_loss: 0.6605 - val_acc: 0.6150\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6591 - acc: 0.6162 - val_loss: 0.6439 - val_acc: 0.6289\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6565 - acc: 0.6190 - val_loss: 0.6672 - val_acc: 0.6029\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6541 - acc: 0.6221 - val_loss: 0.6410 - val_acc: 0.6376\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6527 - acc: 0.6238 - val_loss: 0.6774 - val_acc: 0.6013\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6502 - acc: 0.6274 - val_loss: 0.6339 - val_acc: 0.6445\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6504 - acc: 0.6287 - val_loss: 0.6812 - val_acc: 0.5975\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6475 - acc: 0.6284 - val_loss: 0.6407 - val_acc: 0.6340\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6476 - acc: 0.6291 - val_loss: 0.6339 - val_acc: 0.6422\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6460 - acc: 0.6317 - val_loss: 0.6310 - val_acc: 0.6470\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6460 - acc: 0.6306 - val_loss: 0.6343 - val_acc: 0.6424\n",
      "\n",
      "Accuracy: 64.24%\n",
      "\n",
      "learning rate: 2.5e-05\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 32\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 5.6819 - acc: 0.5368 - val_loss: 0.7258 - val_acc: 0.5552\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6972 - acc: 0.5841 - val_loss: 0.6659 - val_acc: 0.6068\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6803 - acc: 0.6004 - val_loss: 0.7144 - val_acc: 0.5656\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6717 - acc: 0.6091 - val_loss: 0.6480 - val_acc: 0.6305\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6667 - acc: 0.6178 - val_loss: 0.6309 - val_acc: 0.6453\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6638 - acc: 0.6186 - val_loss: 0.6443 - val_acc: 0.6348\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6650 - acc: 0.6205 - val_loss: 0.6360 - val_acc: 0.6451\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6609 - acc: 0.6263 - val_loss: 0.7076 - val_acc: 0.5818\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6588 - acc: 0.6260 - val_loss: 0.6316 - val_acc: 0.6482\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6566 - acc: 0.6282 - val_loss: 0.6285 - val_acc: 0.6522\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6565 - acc: 0.6284 - val_loss: 0.6268 - val_acc: 0.6526\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6586 - acc: 0.6248 - val_loss: 0.6245 - val_acc: 0.6553\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 262us/sample - loss: 0.6554 - acc: 0.6297 - val_loss: 0.6763 - val_acc: 0.6124\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6568 - acc: 0.6276 - val_loss: 0.6469 - val_acc: 0.6344\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6542 - acc: 0.6318 - val_loss: 0.6225 - val_acc: 0.6566\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6512 - acc: 0.6342 - val_loss: 0.6225 - val_acc: 0.6563\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6557 - acc: 0.6307 - val_loss: 0.7657 - val_acc: 0.5549\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6529 - acc: 0.6316 - val_loss: 0.6492 - val_acc: 0.6312\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 264us/sample - loss: 0.6559 - acc: 0.6274 - val_loss: 0.6563 - val_acc: 0.6254\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6523 - acc: 0.6332 - val_loss: 0.6234 - val_acc: 0.6571\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6535 - acc: 0.6321 - val_loss: 0.6245 - val_acc: 0.6550\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 263us/sample - loss: 0.6534 - acc: 0.6319 - val_loss: 0.6691 - val_acc: 0.6170\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6555 - acc: 0.6313 - val_loss: 0.6651 - val_acc: 0.6204\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6524 - acc: 0.6323 - val_loss: 0.6361 - val_acc: 0.6416\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6529 - acc: 0.6297 - val_loss: 0.6259 - val_acc: 0.6546\n",
      "\n",
      "Accuracy: 65.46%\n",
      "\n",
      "learning rate: 2.6e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 31\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 316us/sample - loss: 0.7710 - acc: 0.5173 - val_loss: 0.9263 - val_acc: 0.5018\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.7103 - acc: 0.5557 - val_loss: 0.6538 - val_acc: 0.6232\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6886 - acc: 0.5830 - val_loss: 0.6915 - val_acc: 0.5543\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6756 - acc: 0.5975 - val_loss: 0.6583 - val_acc: 0.6004\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6667 - acc: 0.6095 - val_loss: 0.6306 - val_acc: 0.6531\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6637 - acc: 0.6112 - val_loss: 0.6337 - val_acc: 0.6453\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6611 - acc: 0.6155 - val_loss: 0.6347 - val_acc: 0.6404\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6600 - acc: 0.6164 - val_loss: 0.6436 - val_acc: 0.6306\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6601 - acc: 0.6177 - val_loss: 0.6710 - val_acc: 0.5972\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6586 - acc: 0.6200 - val_loss: 0.7542 - val_acc: 0.5408\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6552 - acc: 0.6259 - val_loss: 0.6264 - val_acc: 0.6558\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6559 - acc: 0.6242 - val_loss: 0.6533 - val_acc: 0.6202\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6537 - acc: 0.6274 - val_loss: 0.6434 - val_acc: 0.6330\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6535 - acc: 0.6264 - val_loss: 0.6229 - val_acc: 0.6572\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6506 - acc: 0.6279 - val_loss: 0.6461 - val_acc: 0.6273\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6522 - acc: 0.6273 - val_loss: 0.6355 - val_acc: 0.6402\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6519 - acc: 0.6281 - val_loss: 0.6435 - val_acc: 0.6330\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6478 - acc: 0.6335 - val_loss: 0.6413 - val_acc: 0.6346\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6491 - acc: 0.6321 - val_loss: 0.6273 - val_acc: 0.6493\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6486 - acc: 0.6317 - val_loss: 0.6237 - val_acc: 0.6551\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6485 - acc: 0.6296 - val_loss: 0.6429 - val_acc: 0.6342\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6444 - acc: 0.6361 - val_loss: 0.6210 - val_acc: 0.6584\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6445 - acc: 0.6350 - val_loss: 0.6395 - val_acc: 0.6387\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6451 - acc: 0.6352 - val_loss: 0.6277 - val_acc: 0.6550\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6445 - acc: 0.6345 - val_loss: 0.6203 - val_acc: 0.6575\n",
      "\n",
      "Accuracy: 65.75%\n",
      "\n",
      "learning rate: 7.1e-06\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 30\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 313us/sample - loss: 1.9350 - acc: 0.4525 - val_loss: 0.7951 - val_acc: 0.4524\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.7619 - acc: 0.4548 - val_loss: 0.7435 - val_acc: 0.4504\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.7406 - acc: 0.4632 - val_loss: 0.7274 - val_acc: 0.4589\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.7268 - acc: 0.4707 - val_loss: 0.7196 - val_acc: 0.4753\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.7175 - acc: 0.4838 - val_loss: 0.7079 - val_acc: 0.4895\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.7090 - acc: 0.5001 - val_loss: 0.7002 - val_acc: 0.5073\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.7020 - acc: 0.5130 - val_loss: 0.6958 - val_acc: 0.5221\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6954 - acc: 0.5281 - val_loss: 0.6882 - val_acc: 0.5421\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6903 - acc: 0.5432 - val_loss: 0.6921 - val_acc: 0.5365\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6853 - acc: 0.5543 - val_loss: 0.6780 - val_acc: 0.5781\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6807 - acc: 0.5657 - val_loss: 0.6776 - val_acc: 0.5698\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6766 - acc: 0.5735 - val_loss: 0.6708 - val_acc: 0.5910\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6725 - acc: 0.5867 - val_loss: 0.6651 - val_acc: 0.6143\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6686 - acc: 0.5935 - val_loss: 0.6633 - val_acc: 0.6074\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6650 - acc: 0.6007 - val_loss: 0.6596 - val_acc: 0.6217\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6623 - acc: 0.6061 - val_loss: 0.6541 - val_acc: 0.6279\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6574 - acc: 0.6169 - val_loss: 0.6503 - val_acc: 0.6373\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6550 - acc: 0.6185 - val_loss: 0.6468 - val_acc: 0.6435\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6516 - acc: 0.6263 - val_loss: 0.6483 - val_acc: 0.6313\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 300us/sample - loss: 0.6486 - acc: 0.6292 - val_loss: 0.6452 - val_acc: 0.6300\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6461 - acc: 0.6305 - val_loss: 0.6571 - val_acc: 0.6051\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6446 - acc: 0.6331 - val_loss: 0.6363 - val_acc: 0.6492\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6423 - acc: 0.6357 - val_loss: 0.6364 - val_acc: 0.6456\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6402 - acc: 0.6390 - val_loss: 0.6420 - val_acc: 0.6328\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6387 - acc: 0.6411 - val_loss: 0.6334 - val_acc: 0.6475\n",
      "\n",
      "Accuracy: 64.75%\n",
      "\n",
      "learning rate: 2.9e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 31\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 1.4793 - acc: 0.4765 - val_loss: 0.7335 - val_acc: 0.5189\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.7581 - acc: 0.5431 - val_loss: 1.1029 - val_acc: 0.4991\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 300us/sample - loss: 0.7203 - acc: 0.5788 - val_loss: 0.7470 - val_acc: 0.5466\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6998 - acc: 0.5975 - val_loss: 0.6481 - val_acc: 0.6303\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 300us/sample - loss: 0.6932 - acc: 0.6058 - val_loss: 0.6421 - val_acc: 0.6364\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6869 - acc: 0.6086 - val_loss: 0.6726 - val_acc: 0.6100\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 300us/sample - loss: 0.6797 - acc: 0.6147 - val_loss: 0.6646 - val_acc: 0.6139\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6775 - acc: 0.6174 - val_loss: 0.6867 - val_acc: 0.5999\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 300us/sample - loss: 0.6747 - acc: 0.6153 - val_loss: 0.6359 - val_acc: 0.6449\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6730 - acc: 0.6199 - val_loss: 0.6625 - val_acc: 0.6215\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6717 - acc: 0.6198 - val_loss: 0.6499 - val_acc: 0.6324\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 300us/sample - loss: 0.6673 - acc: 0.6248 - val_loss: 0.6274 - val_acc: 0.6530\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6740 - acc: 0.6226 - val_loss: 0.7993 - val_acc: 0.5420\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 300us/sample - loss: 0.6749 - acc: 0.6184 - val_loss: 0.6201 - val_acc: 0.6600\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 298us/sample - loss: 0.6679 - acc: 0.6236 - val_loss: 0.6232 - val_acc: 0.6579\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6668 - acc: 0.6224 - val_loss: 0.6320 - val_acc: 0.6460\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 299us/sample - loss: 0.6634 - acc: 0.6254 - val_loss: 0.6654 - val_acc: 0.6147\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6623 - acc: 0.6263 - val_loss: 1.3321 - val_acc: 0.5014\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6636 - acc: 0.6230 - val_loss: 0.6796 - val_acc: 0.6067\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6645 - acc: 0.6240 - val_loss: 0.6447 - val_acc: 0.6346\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6623 - acc: 0.6254 - val_loss: 0.6203 - val_acc: 0.6567\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6594 - acc: 0.6295 - val_loss: 0.6351 - val_acc: 0.6416\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6610 - acc: 0.6276 - val_loss: 0.6320 - val_acc: 0.6473\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6587 - acc: 0.6265 - val_loss: 0.6752 - val_acc: 0.6101\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 299us/sample - loss: 0.6592 - acc: 0.6269 - val_loss: 0.7529 - val_acc: 0.5580\n",
      "\n",
      "Accuracy: 55.80%\n",
      "\n",
      "learning rate: 1.8e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 48\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 312us/sample - loss: 1.0290 - acc: 0.5264 - val_loss: 0.7049 - val_acc: 0.5206\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6895 - acc: 0.5607 - val_loss: 0.6752 - val_acc: 0.5671\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6783 - acc: 0.5802 - val_loss: 0.6692 - val_acc: 0.5843\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6709 - acc: 0.5921 - val_loss: 0.6452 - val_acc: 0.6363\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.6624 - acc: 0.6030 - val_loss: 0.6439 - val_acc: 0.6295\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6590 - acc: 0.6106 - val_loss: 0.6538 - val_acc: 0.6101\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6560 - acc: 0.6144 - val_loss: 0.6755 - val_acc: 0.5803\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.6536 - acc: 0.6187 - val_loss: 0.6430 - val_acc: 0.6277\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.6495 - acc: 0.6225 - val_loss: 0.6306 - val_acc: 0.6472\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6468 - acc: 0.6236 - val_loss: 0.6454 - val_acc: 0.6249\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6470 - acc: 0.6266 - val_loss: 0.6301 - val_acc: 0.6430\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6458 - acc: 0.6294 - val_loss: 0.6916 - val_acc: 0.5762\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 13s 308us/sample - loss: 0.6433 - acc: 0.6337 - val_loss: 0.6254 - val_acc: 0.6495\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6425 - acc: 0.6323 - val_loss: 0.6510 - val_acc: 0.6185\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 308us/sample - loss: 0.6424 - acc: 0.6329 - val_loss: 0.6467 - val_acc: 0.6240\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6420 - acc: 0.6314 - val_loss: 0.6255 - val_acc: 0.6535\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6410 - acc: 0.6315 - val_loss: 0.6567 - val_acc: 0.6203\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 0.6406 - acc: 0.6335 - val_loss: 0.6243 - val_acc: 0.6542\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6387 - acc: 0.6384 - val_loss: 0.6221 - val_acc: 0.6534\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6387 - acc: 0.6364 - val_loss: 0.6292 - val_acc: 0.6436\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 13s 311us/sample - loss: 0.6368 - acc: 0.6401 - val_loss: 0.6480 - val_acc: 0.6257\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6367 - acc: 0.6386 - val_loss: 0.6269 - val_acc: 0.6475\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 13s 314us/sample - loss: 0.6364 - acc: 0.6380 - val_loss: 0.6292 - val_acc: 0.6426\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 307us/sample - loss: 0.6353 - acc: 0.6379 - val_loss: 0.6294 - val_acc: 0.6457\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 13s 310us/sample - loss: 0.6344 - acc: 0.6388 - val_loss: 0.6452 - val_acc: 0.6254\n",
      "\n",
      "Accuracy: 62.54%\n",
      "\n",
      "learning rate: 1.0e-06\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 31\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 13s 309us/sample - loss: 2.8593 - acc: 0.4930 - val_loss: 0.7316 - val_acc: 0.4943\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.7330 - acc: 0.4857 - val_loss: 0.7223 - val_acc: 0.4933\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.7258 - acc: 0.4850 - val_loss: 0.7174 - val_acc: 0.4948\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.7210 - acc: 0.4897 - val_loss: 0.7133 - val_acc: 0.5000\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.7169 - acc: 0.4930 - val_loss: 0.7092 - val_acc: 0.5061\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.7124 - acc: 0.5013 - val_loss: 0.7054 - val_acc: 0.5136\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.7089 - acc: 0.5066 - val_loss: 0.7020 - val_acc: 0.5181\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.7056 - acc: 0.5094 - val_loss: 0.6990 - val_acc: 0.5224\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.7025 - acc: 0.5172 - val_loss: 0.6976 - val_acc: 0.5288\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6994 - acc: 0.5190 - val_loss: 0.6932 - val_acc: 0.5338\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6968 - acc: 0.5274 - val_loss: 0.6908 - val_acc: 0.5370\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6944 - acc: 0.5314 - val_loss: 0.6890 - val_acc: 0.5470\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6922 - acc: 0.5348 - val_loss: 0.6864 - val_acc: 0.5493\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6895 - acc: 0.5394 - val_loss: 0.6859 - val_acc: 0.5559\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6875 - acc: 0.5461 - val_loss: 0.6825 - val_acc: 0.5583\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 12s 298us/sample - loss: 0.6854 - acc: 0.5502 - val_loss: 0.6819 - val_acc: 0.5678\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6836 - acc: 0.5565 - val_loss: 0.6785 - val_acc: 0.5699\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 300us/sample - loss: 0.6816 - acc: 0.5637 - val_loss: 0.6769 - val_acc: 0.5770\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6800 - acc: 0.5680 - val_loss: 0.6781 - val_acc: 0.5676\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 0.6781 - acc: 0.5728 - val_loss: 0.6740 - val_acc: 0.5821\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6764 - acc: 0.5766 - val_loss: 0.6733 - val_acc: 0.5820\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6753 - acc: 0.5803 - val_loss: 0.6715 - val_acc: 0.5954\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6736 - acc: 0.5868 - val_loss: 0.6703 - val_acc: 0.5972\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6722 - acc: 0.5879 - val_loss: 0.6680 - val_acc: 0.6052\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 300us/sample - loss: 0.6706 - acc: 0.5917 - val_loss: 0.6713 - val_acc: 0.5887\n",
      "\n",
      "Accuracy: 58.87%\n",
      "\n",
      "learning rate: 1.0e-02\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 48\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.8722 - acc: 0.5027 - val_loss: 0.6937 - val_acc: 0.4984\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6938 - acc: 0.4956 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6937 - acc: 0.5016 - val_loss: 0.6954 - val_acc: 0.4984\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6938 - acc: 0.4959 - val_loss: 0.6935 - val_acc: 0.5016\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6937 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6938 - acc: 0.4983 - val_loss: 0.6934 - val_acc: 0.4984\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6937 - acc: 0.4986 - val_loss: 0.6939 - val_acc: 0.5016\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6938 - acc: 0.4983 - val_loss: 0.6934 - val_acc: 0.5016\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6938 - acc: 0.4993 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6938 - acc: 0.4966 - val_loss: 0.6934 - val_acc: 0.4984\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6938 - acc: 0.4959 - val_loss: 0.6935 - val_acc: 0.5016\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6937 - acc: 0.4996 - val_loss: 0.6933 - val_acc: 0.5016\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6937 - acc: 0.4982 - val_loss: 0.6956 - val_acc: 0.5016\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6938 - acc: 0.4960 - val_loss: 0.6934 - val_acc: 0.5016\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6938 - acc: 0.4963 - val_loss: 0.6938 - val_acc: 0.4984\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6937 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5016\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6937 - acc: 0.5022 - val_loss: 0.6935 - val_acc: 0.5016\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6937 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6936 - acc: 0.5007 - val_loss: 0.6934 - val_acc: 0.5016\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6938 - acc: 0.4978 - val_loss: 0.6936 - val_acc: 0.5016\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6936 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6936 - acc: 0.5022 - val_loss: 0.6935 - val_acc: 0.4984\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6936 - acc: 0.5036 - val_loss: 0.6933 - val_acc: 0.4984\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6937 - acc: 0.5010 - val_loss: 0.6934 - val_acc: 0.4984\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6937 - acc: 0.4990 - val_loss: 0.6934 - val_acc: 0.5016\n",
      "\n",
      "Accuracy: 50.16%\n",
      "\n",
      "learning rate: 1.8e-05\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 48\n",
      "activation: relu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 277us/sample - loss: 1.2388 - acc: 0.5232 - val_loss: 0.8068 - val_acc: 0.5416\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.7649 - acc: 0.5568 - val_loss: 0.6969 - val_acc: 0.5827\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.7220 - acc: 0.5793 - val_loss: 0.6772 - val_acc: 0.6046\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.7037 - acc: 0.5951 - val_loss: 0.6584 - val_acc: 0.6205\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6911 - acc: 0.6035 - val_loss: 0.6732 - val_acc: 0.6114\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6815 - acc: 0.6093 - val_loss: 0.6683 - val_acc: 0.6140\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6760 - acc: 0.6134 - val_loss: 0.6493 - val_acc: 0.6348\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6697 - acc: 0.6190 - val_loss: 0.6363 - val_acc: 0.6443\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6656 - acc: 0.6223 - val_loss: 0.7602 - val_acc: 0.5610\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6679 - acc: 0.6226 - val_loss: 0.6292 - val_acc: 0.6489\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6631 - acc: 0.6230 - val_loss: 0.6469 - val_acc: 0.6366\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6610 - acc: 0.6271 - val_loss: 0.6269 - val_acc: 0.6505\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6600 - acc: 0.6258 - val_loss: 0.6521 - val_acc: 0.6277\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6625 - acc: 0.6256 - val_loss: 0.6281 - val_acc: 0.6489\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6563 - acc: 0.6306 - val_loss: 0.6227 - val_acc: 0.6543\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6572 - acc: 0.6283 - val_loss: 0.6527 - val_acc: 0.6275\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6578 - acc: 0.6299 - val_loss: 0.6204 - val_acc: 0.6574\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6589 - acc: 0.6294 - val_loss: 0.7040 - val_acc: 0.5934\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6533 - acc: 0.6328 - val_loss: 0.6348 - val_acc: 0.6392\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6588 - acc: 0.6285 - val_loss: 0.6828 - val_acc: 0.6061\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6563 - acc: 0.6311 - val_loss: 0.8873 - val_acc: 0.5201\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6556 - acc: 0.6321 - val_loss: 0.6629 - val_acc: 0.6190\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6549 - acc: 0.6315 - val_loss: 0.6189 - val_acc: 0.6588\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 273us/sample - loss: 0.6548 - acc: 0.6300 - val_loss: 0.6218 - val_acc: 0.6551\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6575 - acc: 0.6284 - val_loss: 0.6583 - val_acc: 0.6244\n",
      "\n",
      "Accuracy: 62.44%\n",
      "\n",
      "learning rate: 6.8e-06\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 31\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 8.1783 - acc: 0.4949 - val_loss: 0.9021 - val_acc: 0.5008\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.8299 - acc: 0.5102 - val_loss: 0.7923 - val_acc: 0.5224\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.7890 - acc: 0.5236 - val_loss: 0.7604 - val_acc: 0.5373\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.7632 - acc: 0.5395 - val_loss: 0.7395 - val_acc: 0.5498\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.7425 - acc: 0.5537 - val_loss: 0.7238 - val_acc: 0.5634\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.7186 - acc: 0.5683 - val_loss: 0.7192 - val_acc: 0.5733\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6907 - acc: 0.5858 - val_loss: 0.6705 - val_acc: 0.6058\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6786 - acc: 0.5978 - val_loss: 0.6596 - val_acc: 0.6173\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6704 - acc: 0.6079 - val_loss: 0.6530 - val_acc: 0.6258\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6646 - acc: 0.6108 - val_loss: 0.6489 - val_acc: 0.6268\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 272us/sample - loss: 0.6598 - acc: 0.6163 - val_loss: 0.6534 - val_acc: 0.6276\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6567 - acc: 0.6188 - val_loss: 0.6415 - val_acc: 0.6353\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6536 - acc: 0.6232 - val_loss: 0.6382 - val_acc: 0.6391\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6503 - acc: 0.6268 - val_loss: 0.6437 - val_acc: 0.6344\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6488 - acc: 0.6309 - val_loss: 0.6402 - val_acc: 0.6412\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6474 - acc: 0.6289 - val_loss: 0.6487 - val_acc: 0.6322\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6453 - acc: 0.6324 - val_loss: 0.6306 - val_acc: 0.6473\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6431 - acc: 0.6341 - val_loss: 0.6459 - val_acc: 0.6338\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6424 - acc: 0.6390 - val_loss: 0.6429 - val_acc: 0.6342\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6412 - acc: 0.6377 - val_loss: 0.6332 - val_acc: 0.6477\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6412 - acc: 0.6370 - val_loss: 0.6298 - val_acc: 0.6493\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6389 - acc: 0.6386 - val_loss: 0.6315 - val_acc: 0.6473\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6383 - acc: 0.6406 - val_loss: 0.6458 - val_acc: 0.6307\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6378 - acc: 0.6429 - val_loss: 0.6515 - val_acc: 0.6262\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6360 - acc: 0.6419 - val_loss: 0.6255 - val_acc: 0.6540\n",
      "\n",
      "Accuracy: 65.40%\n",
      "\n",
      "learning rate: 6.5e-06\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 31\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 12s 306us/sample - loss: 2.5262 - acc: 0.5015 - val_loss: 0.7000 - val_acc: 0.5144\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6961 - acc: 0.5338 - val_loss: 0.7057 - val_acc: 0.5185\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 12s 300us/sample - loss: 0.6860 - acc: 0.5548 - val_loss: 0.6727 - val_acc: 0.5894\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6789 - acc: 0.5678 - val_loss: 0.6655 - val_acc: 0.6112\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6721 - acc: 0.5821 - val_loss: 0.6620 - val_acc: 0.6060\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6665 - acc: 0.5983 - val_loss: 0.6555 - val_acc: 0.6272\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6639 - acc: 0.6016 - val_loss: 0.6511 - val_acc: 0.6372\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6591 - acc: 0.6089 - val_loss: 0.6923 - val_acc: 0.5421\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6559 - acc: 0.6123 - val_loss: 0.6509 - val_acc: 0.6228\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6536 - acc: 0.6158 - val_loss: 0.6515 - val_acc: 0.6171\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6519 - acc: 0.6207 - val_loss: 0.6421 - val_acc: 0.6362\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6500 - acc: 0.6241 - val_loss: 0.6398 - val_acc: 0.6425\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6484 - acc: 0.6226 - val_loss: 0.6682 - val_acc: 0.5817\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6464 - acc: 0.6270 - val_loss: 0.6364 - val_acc: 0.6485\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6458 - acc: 0.6275 - val_loss: 0.6355 - val_acc: 0.6476\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 12s 304us/sample - loss: 0.6444 - acc: 0.6311 - val_loss: 0.6343 - val_acc: 0.6466\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6425 - acc: 0.6301 - val_loss: 0.6330 - val_acc: 0.6447\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6423 - acc: 0.6307 - val_loss: 0.6327 - val_acc: 0.6440\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6411 - acc: 0.6315 - val_loss: 0.6454 - val_acc: 0.6249\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 12s 305us/sample - loss: 0.6405 - acc: 0.6329 - val_loss: 0.6302 - val_acc: 0.6475\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 12s 300us/sample - loss: 0.6391 - acc: 0.6364 - val_loss: 0.6304 - val_acc: 0.6459\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 12s 303us/sample - loss: 0.6396 - acc: 0.6351 - val_loss: 0.6428 - val_acc: 0.6290\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 12s 300us/sample - loss: 0.6398 - acc: 0.6346 - val_loss: 0.6367 - val_acc: 0.6377\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 12s 302us/sample - loss: 0.6383 - acc: 0.6354 - val_loss: 0.6427 - val_acc: 0.6290\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 12s 301us/sample - loss: 0.6371 - acc: 0.6403 - val_loss: 0.6274 - val_acc: 0.6489\n",
      "\n",
      "Accuracy: 64.89%\n",
      "\n",
      "learning rate: 6.6e-06\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 31\n",
      "activation: elu\n",
      "Train on 40561 samples, validate on 10141 samples\n",
      "Epoch 1/25\n",
      "40561/40561 [==============================] - 11s 274us/sample - loss: 0.9933 - acc: 0.5683 - val_loss: 0.7552 - val_acc: 0.5722\n",
      "Epoch 2/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.7540 - acc: 0.5812 - val_loss: 0.7338 - val_acc: 0.5804\n",
      "Epoch 3/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.7403 - acc: 0.5849 - val_loss: 0.7581 - val_acc: 0.5764\n",
      "Epoch 4/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.7271 - acc: 0.5918 - val_loss: 0.7170 - val_acc: 0.5941\n",
      "Epoch 5/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.7134 - acc: 0.5978 - val_loss: 0.7000 - val_acc: 0.5999\n",
      "Epoch 6/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.7057 - acc: 0.6019 - val_loss: 0.6893 - val_acc: 0.6085\n",
      "Epoch 7/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6985 - acc: 0.6045 - val_loss: 0.6795 - val_acc: 0.6096\n",
      "Epoch 8/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6897 - acc: 0.6116 - val_loss: 0.6729 - val_acc: 0.6176\n",
      "Epoch 9/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6842 - acc: 0.6130 - val_loss: 0.6797 - val_acc: 0.6094\n",
      "Epoch 10/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6789 - acc: 0.6147 - val_loss: 0.6607 - val_acc: 0.6261\n",
      "Epoch 11/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6736 - acc: 0.6193 - val_loss: 0.6598 - val_acc: 0.6241\n",
      "Epoch 12/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6693 - acc: 0.6226 - val_loss: 0.6525 - val_acc: 0.6299\n",
      "Epoch 13/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6663 - acc: 0.6224 - val_loss: 0.6499 - val_acc: 0.6324\n",
      "Epoch 14/25\n",
      "40561/40561 [==============================] - 11s 269us/sample - loss: 0.6641 - acc: 0.6236 - val_loss: 0.6624 - val_acc: 0.6199\n",
      "Epoch 15/25\n",
      "40561/40561 [==============================] - 11s 265us/sample - loss: 0.6607 - acc: 0.6258 - val_loss: 0.6929 - val_acc: 0.6051\n",
      "Epoch 16/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6568 - acc: 0.6274 - val_loss: 0.6581 - val_acc: 0.6207\n",
      "Epoch 17/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6558 - acc: 0.6301 - val_loss: 0.6454 - val_acc: 0.6339\n",
      "Epoch 18/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6525 - acc: 0.6306 - val_loss: 0.6399 - val_acc: 0.6401\n",
      "Epoch 19/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6515 - acc: 0.6311 - val_loss: 0.6483 - val_acc: 0.6328\n",
      "Epoch 20/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6502 - acc: 0.6314 - val_loss: 0.6776 - val_acc: 0.6058\n",
      "Epoch 21/25\n",
      "40561/40561 [==============================] - 11s 267us/sample - loss: 0.6491 - acc: 0.6355 - val_loss: 0.6789 - val_acc: 0.6080\n",
      "Epoch 22/25\n",
      "40561/40561 [==============================] - 11s 266us/sample - loss: 0.6487 - acc: 0.6349 - val_loss: 0.6316 - val_acc: 0.6459\n",
      "Epoch 23/25\n",
      "40561/40561 [==============================] - 11s 270us/sample - loss: 0.6447 - acc: 0.6362 - val_loss: 0.6302 - val_acc: 0.6473\n",
      "Epoch 24/25\n",
      "40561/40561 [==============================] - 11s 268us/sample - loss: 0.6464 - acc: 0.6339 - val_loss: 0.6306 - val_acc: 0.6482\n",
      "Epoch 25/25\n",
      "40561/40561 [==============================] - 11s 271us/sample - loss: 0.6436 - acc: 0.6383 - val_loss: 0.6291 - val_acc: 0.6484\n",
      "\n",
      "Accuracy: 64.84%\n",
      "\n",
      "CPU times: user 5h 42min 13s, sys: 56min 42s, total: 6h 38min 55s\n",
      "Wall time: 4h 3min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=hyperparameters,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=50,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4886297981400537e-05, 1, 2, 'relu']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6600927"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-search_result.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_objective_2D(result=search_result,\n",
    "                        dimension_identifier1='learning_rate',\n",
    "                        dimension_identifier2='num_dense_layers',\n",
    "                        levels=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3c97aff12480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdim_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num_dense_nodes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num_dense_layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim_names = ['learning_rate', 'num_dense_nodes', 'num_dense_layers']\n",
    "fig, ax = plot_objective(result=search_result, plot_dims=dim_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_evaluations(result=search_result, plot_dims=dim_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
